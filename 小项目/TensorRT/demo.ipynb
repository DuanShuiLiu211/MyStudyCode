{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1：基准测试 pytorch2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanghao/program/miniconda3/envs/tensor/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch 推理结果： tensor([904, 490, 490, 490, 904, 490, 490, 490], device='cuda:0')\n",
      "Pytorch 推理时间： 0.0043642823696136476\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import time\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 导出模型\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# 3. 输入数据\n",
    "raw_data = torch.randn((8, 3, 224, 224), dtype=torch.float32, device=\"cpu\")\n",
    "input_tensor = raw_data.to(\"cuda\")\n",
    "\n",
    "# 4. 进行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    output = model(input_tensor)\n",
    "time_start = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "time_end = time.time()\n",
    "print(\"Pytorch 推理结果：\", predicted_class)\n",
    "print(\"Pytorch 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2：使用 ONNX1.7 优化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 推理结果： [904 490 490 490 904 490 490 490]\n",
      "ONNX 推理时间： 0.0043642823696136476\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 转换并导出静态和动态模型\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(8, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model.onnx\",\n",
    ")\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model_dynamic.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输入张量的名称是'input'，仅固定通道维度\n",
    "        \"output\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输出张量的名称是'output'，仅固定通道维度\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. 输入数据\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "\n",
    "# 3. 创建推理会话\n",
    "tensorrt_accelerate = False\n",
    "if tensorrt_accelerate:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    providers = [\n",
    "        \"TensorrtExecutionProvider\",\n",
    "        \"CUDAExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "else:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    sess_options.graph_optimization_level = (\n",
    "        ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    )  # 启用所有适用的图优化技术\n",
    "    providers = [\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "session = ort.InferenceSession(\"model_dynamic.onnx\", sess_options, providers=providers)\n",
    "\n",
    "# 4. 执行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for i in range(num):\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "    predicted_class = np.argmax(output_data[0], axis=1)\n",
    "end_time = time.time()\n",
    "print(\"ONNX 推理结果：\", predicted_class)\n",
    "print(\"ONNX 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3：使用 TensorRT8.6 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch\n",
      "[02/05/2024-14:33:50] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[02/05/2024-14:33:50] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[02/05/2024-14:33:50] [I] === Model Options ===\n",
      "[02/05/2024-14:33:50] [I] Format: ONNX\n",
      "[02/05/2024-14:33:50] [I] Model: model_dynamic.onnx\n",
      "[02/05/2024-14:33:50] [I] Output:\n",
      "[02/05/2024-14:33:50] [I] === Build Options ===\n",
      "[02/05/2024-14:33:50] [I] Max batch: explicit batch\n",
      "[02/05/2024-14:33:50] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/05/2024-14:33:50] [I] minTiming: 1\n",
      "[02/05/2024-14:33:50] [I] avgTiming: 8\n",
      "[02/05/2024-14:33:50] [I] Precision: FP32\n",
      "[02/05/2024-14:33:50] [I] LayerPrecisions: \n",
      "[02/05/2024-14:33:50] [I] Layer Device Types: \n",
      "[02/05/2024-14:33:50] [I] Calibration: \n",
      "[02/05/2024-14:33:50] [I] Refit: Disabled\n",
      "[02/05/2024-14:33:50] [I] Version Compatible: Disabled\n",
      "[02/05/2024-14:33:50] [I] TensorRT runtime: full\n",
      "[02/05/2024-14:33:50] [I] Lean DLL Path: \n",
      "[02/05/2024-14:33:50] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/05/2024-14:33:50] [I] Exclude Lean Runtime: Disabled\n",
      "[02/05/2024-14:33:50] [I] Sparsity: Disabled\n",
      "[02/05/2024-14:33:50] [I] Safe mode: Disabled\n",
      "[02/05/2024-14:33:50] [I] Build DLA standalone loadable: Disabled\n",
      "[02/05/2024-14:33:50] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/05/2024-14:33:50] [I] DirectIO mode: Disabled\n",
      "[02/05/2024-14:33:50] [I] Restricted mode: Disabled\n",
      "[02/05/2024-14:33:50] [I] Skip inference: Disabled\n",
      "[02/05/2024-14:33:50] [I] Save engine: model_dynamic.trt\n",
      "[02/05/2024-14:33:50] [I] Load engine: \n",
      "[02/05/2024-14:33:50] [I] Profiling verbosity: 0\n",
      "[02/05/2024-14:33:50] [I] Tactic sources: Using default tactic sources\n",
      "[02/05/2024-14:33:50] [I] timingCacheMode: local\n",
      "[02/05/2024-14:33:50] [I] timingCacheFile: \n",
      "[02/05/2024-14:33:50] [I] Heuristic: Disabled\n",
      "[02/05/2024-14:33:50] [I] Preview Features: Use default preview flags.\n",
      "[02/05/2024-14:33:50] [I] MaxAuxStreams: -1\n",
      "[02/05/2024-14:33:50] [I] BuilderOptimizationLevel: -1\n",
      "[02/05/2024-14:33:50] [I] Input(s)s format: fp32:CHW\n",
      "[02/05/2024-14:33:50] [I] Output(s)s format: fp32:CHW\n",
      "[02/05/2024-14:33:50] [I] Input build shape: input=1x3x112x112+4x3x224x224+8x3x448x448\n",
      "[02/05/2024-14:33:50] [I] Input calibration shapes: model\n",
      "[02/05/2024-14:33:50] [I] === System Options ===\n",
      "[02/05/2024-14:33:50] [I] Device: 0\n",
      "[02/05/2024-14:33:50] [I] DLACore: \n",
      "[02/05/2024-14:33:50] [I] Plugins:\n",
      "[02/05/2024-14:33:50] [I] setPluginsToSerialize:\n",
      "[02/05/2024-14:33:50] [I] dynamicPlugins:\n",
      "[02/05/2024-14:33:50] [I] ignoreParsedPluginLibs: 0\n",
      "[02/05/2024-14:33:50] [I] \n",
      "[02/05/2024-14:33:50] [I] === Inference Options ===\n",
      "[02/05/2024-14:33:50] [I] Batch: Explicit\n",
      "[02/05/2024-14:33:50] [I] Input inference shape: input=4x3x224x224\n",
      "[02/05/2024-14:33:50] [I] Iterations: 10\n",
      "[02/05/2024-14:33:50] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/05/2024-14:33:50] [I] Sleep time: 0ms\n",
      "[02/05/2024-14:33:50] [I] Idle time: 0ms\n",
      "[02/05/2024-14:33:50] [I] Inference Streams: 1\n",
      "[02/05/2024-14:33:50] [I] ExposeDMA: Disabled\n",
      "[02/05/2024-14:33:50] [I] Data transfers: Enabled\n",
      "[02/05/2024-14:33:50] [I] Spin-wait: Disabled\n",
      "[02/05/2024-14:33:50] [I] Multithreading: Disabled\n",
      "[02/05/2024-14:33:50] [I] CUDA Graph: Disabled\n",
      "[02/05/2024-14:33:50] [I] Separate profiling: Disabled\n",
      "[02/05/2024-14:33:50] [I] Time Deserialize: Disabled\n",
      "[02/05/2024-14:33:50] [I] Time Refit: Disabled\n",
      "[02/05/2024-14:33:50] [I] NVTX verbosity: 0\n",
      "[02/05/2024-14:33:50] [I] Persistent Cache Ratio: 0\n",
      "[02/05/2024-14:33:50] [I] Inputs:\n",
      "[02/05/2024-14:33:50] [I] === Reporting Options ===\n",
      "[02/05/2024-14:33:50] [I] Verbose: Disabled\n",
      "[02/05/2024-14:33:50] [I] Averages: 10 inferences\n",
      "[02/05/2024-14:33:50] [I] Percentiles: 90,95,99\n",
      "[02/05/2024-14:33:50] [I] Dump refittable layers:Disabled\n",
      "[02/05/2024-14:33:50] [I] Dump output: Disabled\n",
      "[02/05/2024-14:33:50] [I] Profile: Disabled\n",
      "[02/05/2024-14:33:50] [I] Export timing to JSON file: \n",
      "[02/05/2024-14:33:50] [I] Export output to JSON file: \n",
      "[02/05/2024-14:33:50] [I] Export profile to JSON file: \n",
      "[02/05/2024-14:33:50] [I] \n",
      "[02/05/2024-14:33:50] [I] === Device Information ===\n",
      "[02/05/2024-14:33:50] [I] Selected Device: NVIDIA GeForce RTX 4080\n",
      "[02/05/2024-14:33:50] [I] Compute Capability: 8.9\n",
      "[02/05/2024-14:33:50] [I] SMs: 76\n",
      "[02/05/2024-14:33:50] [I] Device Global Memory: 16375 MiB\n",
      "[02/05/2024-14:33:50] [I] Shared Memory per SM: 100 KiB\n",
      "[02/05/2024-14:33:50] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[02/05/2024-14:33:50] [I] Application Compute Clock Rate: 2.505 GHz\n",
      "[02/05/2024-14:33:50] [I] Application Memory Clock Rate: 11.201 GHz\n",
      "[02/05/2024-14:33:50] [I] \n",
      "[02/05/2024-14:33:50] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/05/2024-14:33:50] [I] \n",
      "[02/05/2024-14:33:50] [I] TensorRT version: 8.6.1\n",
      "[02/05/2024-14:33:50] [I] Loading standard plugins\n",
      "[02/05/2024-14:33:51] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 21, GPU 1313 (MiB)\n",
      "[02/05/2024-14:33:55] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1446, GPU +268, now: CPU 1544, GPU 1581 (MiB)\n",
      "[02/05/2024-14:33:55] [I] Start parsing network model.\n",
      "[02/05/2024-14:33:55] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/05/2024-14:33:55] [I] [TRT] Input filename:   model_dynamic.onnx\n",
      "[02/05/2024-14:33:55] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/05/2024-14:33:55] [I] [TRT] Opset version:    17\n",
      "[02/05/2024-14:33:55] [I] [TRT] Producer name:    pytorch\n",
      "[02/05/2024-14:33:55] [I] [TRT] Producer version: 2.2.0\n",
      "[02/05/2024-14:33:55] [I] [TRT] Domain:           \n",
      "[02/05/2024-14:33:55] [I] [TRT] Model version:    0\n",
      "[02/05/2024-14:33:55] [I] [TRT] Doc string:       \n",
      "[02/05/2024-14:33:55] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/05/2024-14:33:55] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/05/2024-14:33:55] [I] Finished parsing network model. Parse time: 0.16483\n",
      "[02/05/2024-14:33:55] [I] [TRT] Graph optimization time: 0.018549 seconds.\n",
      "[02/05/2024-14:33:55] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/05/2024-14:34:19] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/05/2024-14:34:20] [I] [TRT] Total Host Persistent Memory: 318864\n",
      "[02/05/2024-14:34:20] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[02/05/2024-14:34:20] [I] [TRT] Total Scratch Memory: 38541824\n",
      "[02/05/2024-14:34:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 103 MiB, GPU 255 MiB\n",
      "[02/05/2024-14:34:20] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 117 steps to complete.\n",
      "[02/05/2024-14:34:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.13757ms to assign 12 blocks to 117 nodes requiring 282610688 bytes.\n",
      "[02/05/2024-14:34:20] [I] [TRT] Total Activation Memory: 282608640\n",
      "[02/05/2024-14:34:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +79, GPU +122, now: CPU 79, GPU 122 (MiB)\n",
      "[02/05/2024-14:34:20] [I] Engine built in 29.2831 sec.\n",
      "[02/05/2024-14:34:20] [I] [TRT] Loaded engine size: 124 MiB\n",
      "[02/05/2024-14:34:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +121, now: CPU 0, GPU 121 (MiB)\n",
      "[02/05/2024-14:34:20] [I] Engine deserialized in 0.0188727 sec.\n",
      "[02/05/2024-14:34:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +270, now: CPU 0, GPU 391 (MiB)\n",
      "[02/05/2024-14:34:20] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/05/2024-14:34:20] [I] Using random values for input input\n",
      "[02/05/2024-14:34:20] [I] Input binding for input with dimensions 4x3x224x224 is created.\n",
      "[02/05/2024-14:34:20] [I] Output binding for output with dimensions 4x1000 is created.\n",
      "[02/05/2024-14:34:20] [I] Starting inference\n",
      "[02/05/2024-14:34:23] [I] Warmup completed 105 queries over 200 ms\n",
      "[02/05/2024-14:34:23] [I] Timing trace has 1559 queries over 3.00377 s\n",
      "[02/05/2024-14:34:23] [I] \n",
      "[02/05/2024-14:34:23] [I] === Trace details ===\n",
      "[02/05/2024-14:34:23] [I] Trace averages of 10 runs:\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76273 ms - Host latency: 1.85851 ms (enqueue 0.431956 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82525 ms - Host latency: 1.9212 ms (enqueue 0.912862 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76502 ms - Host latency: 1.86086 ms (enqueue 0.961275 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78253 ms - Host latency: 1.87838 ms (enqueue 0.898953 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74265 ms - Host latency: 1.8384 ms (enqueue 0.71329 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7613 ms - Host latency: 1.85805 ms (enqueue 0.400696 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81714 ms - Host latency: 1.91307 ms (enqueue 0.421939 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74228 ms - Host latency: 1.83821 ms (enqueue 0.430667 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76101 ms - Host latency: 1.85785 ms (enqueue 0.428528 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76166 ms - Host latency: 1.85751 ms (enqueue 0.414239 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74279 ms - Host latency: 1.83867 ms (enqueue 0.42955 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.83254 ms - Host latency: 1.92879 ms (enqueue 0.990256 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74251 ms - Host latency: 1.8384 ms (enqueue 0.434863 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78055 ms - Host latency: 1.87635 ms (enqueue 0.431488 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76225 ms - Host latency: 1.85806 ms (enqueue 0.433542 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76267 ms - Host latency: 1.8586 ms (enqueue 0.41647 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81514 ms - Host latency: 1.91124 ms (enqueue 0.425922 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76225 ms - Host latency: 1.85817 ms (enqueue 0.416797 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76208 ms - Host latency: 1.858 ms (enqueue 0.449042 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76208 ms - Host latency: 1.85792 ms (enqueue 1.78328 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76429 ms - Host latency: 1.8601 ms (enqueue 1.8428 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81909 ms - Host latency: 1.91522 ms (enqueue 1.79726 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76281 ms - Host latency: 1.85865 ms (enqueue 1.85627 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.77165 ms - Host latency: 1.86754 ms (enqueue 1.81179 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.77565 ms - Host latency: 1.87148 ms (enqueue 1.81686 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78443 ms - Host latency: 1.88023 ms (enqueue 1.80237 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82187 ms - Host latency: 1.91788 ms (enqueue 1.7722 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76175 ms - Host latency: 1.85757 ms (enqueue 1.88265 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76425 ms - Host latency: 1.86014 ms (enqueue 1.83412 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76505 ms - Host latency: 1.86089 ms (enqueue 1.81238 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74667 ms - Host latency: 1.84259 ms (enqueue 1.79464 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76709 ms - Host latency: 1.86291 ms (enqueue 1.80958 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.79585 ms - Host latency: 1.89187 ms (enqueue 0.57558 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74246 ms - Host latency: 1.83839 ms (enqueue 0.516559 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7625 ms - Host latency: 1.85836 ms (enqueue 0.781018 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78162 ms - Host latency: 1.87747 ms (enqueue 0.432245 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74267 ms - Host latency: 1.83843 ms (enqueue 0.418036 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.94272 ms - Host latency: 2.03864 ms (enqueue 0.495648 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76167 ms - Host latency: 1.85764 ms (enqueue 0.951898 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76221 ms - Host latency: 1.85804 ms (enqueue 0.899377 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76266 ms - Host latency: 1.85846 ms (enqueue 0.566846 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81488 ms - Host latency: 1.91214 ms (enqueue 0.498688 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76385 ms - Host latency: 1.85969 ms (enqueue 0.422443 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76317 ms - Host latency: 1.85913 ms (enqueue 0.432202 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76582 ms - Host latency: 1.86169 ms (enqueue 0.720911 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78109 ms - Host latency: 1.87693 ms (enqueue 0.930212 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81812 ms - Host latency: 1.91412 ms (enqueue 0.822974 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76166 ms - Host latency: 1.8575 ms (enqueue 0.436133 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76155 ms - Host latency: 1.85739 ms (enqueue 0.422717 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76113 ms - Host latency: 1.85691 ms (enqueue 0.449927 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76117 ms - Host latency: 1.85693 ms (enqueue 0.518201 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.772 ms - Host latency: 1.86844 ms (enqueue 0.99259 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76511 ms - Host latency: 1.86102 ms (enqueue 1.04679 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76283 ms - Host latency: 1.85872 ms (enqueue 0.526636 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76272 ms - Host latency: 1.85857 ms (enqueue 0.397913 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78158 ms - Host latency: 1.87743 ms (enqueue 0.408875 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7959 ms - Host latency: 1.8918 ms (enqueue 0.70907 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7521 ms - Host latency: 1.86151 ms (enqueue 1.72167 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81304 ms - Host latency: 1.90925 ms (enqueue 1.84359 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.77742 ms - Host latency: 1.87325 ms (enqueue 1.83251 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7665 ms - Host latency: 1.86235 ms (enqueue 1.82435 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.83062 ms - Host latency: 1.92648 ms (enqueue 1.89332 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.77179 ms - Host latency: 1.86774 ms (enqueue 1.49878 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76306 ms - Host latency: 1.85892 ms (enqueue 0.427893 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76538 ms - Host latency: 1.86123 ms (enqueue 0.643945 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76514 ms - Host latency: 1.86093 ms (enqueue 0.888916 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.80676 ms - Host latency: 1.90261 ms (enqueue 0.865161 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82009 ms - Host latency: 1.91602 ms (enqueue 0.425781 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76475 ms - Host latency: 1.86057 ms (enqueue 0.418481 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7647 ms - Host latency: 1.86053 ms (enqueue 0.828247 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76477 ms - Host latency: 1.86077 ms (enqueue 0.953381 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81841 ms - Host latency: 1.91433 ms (enqueue 0.894226 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76774 ms - Host latency: 1.8636 ms (enqueue 0.93186 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7671 ms - Host latency: 1.86298 ms (enqueue 0.920056 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74564 ms - Host latency: 1.84161 ms (enqueue 0.468945 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7641 ms - Host latency: 1.86 ms (enqueue 0.418286 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.84146 ms - Host latency: 1.93748 ms (enqueue 0.435901 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76549 ms - Host latency: 1.86134 ms (enqueue 1.38326 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76819 ms - Host latency: 1.86403 ms (enqueue 1.84106 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74911 ms - Host latency: 1.84502 ms (enqueue 1.79387 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82899 ms - Host latency: 1.9249 ms (enqueue 1.86834 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76317 ms - Host latency: 1.85895 ms (enqueue 0.888013 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81992 ms - Host latency: 1.91613 ms (enqueue 1.5042 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76827 ms - Host latency: 1.86422 ms (enqueue 1.83033 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76558 ms - Host latency: 1.86144 ms (enqueue 1.81128 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78593 ms - Host latency: 1.88185 ms (enqueue 1.82882 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7657 ms - Host latency: 1.8616 ms (enqueue 0.71615 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.80127 ms - Host latency: 1.8973 ms (enqueue 0.425525 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76462 ms - Host latency: 1.86046 ms (enqueue 0.539551 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76405 ms - Host latency: 1.85981 ms (enqueue 0.746899 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76461 ms - Host latency: 1.86039 ms (enqueue 0.735242 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76592 ms - Host latency: 1.86174 ms (enqueue 0.926562 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82285 ms - Host latency: 1.91887 ms (enqueue 1.04218 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76549 ms - Host latency: 1.86139 ms (enqueue 0.790894 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76644 ms - Host latency: 1.86232 ms (enqueue 0.434094 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74718 ms - Host latency: 1.84304 ms (enqueue 0.433496 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76589 ms - Host latency: 1.86174 ms (enqueue 0.570898 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82051 ms - Host latency: 1.91636 ms (enqueue 0.570923 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78574 ms - Host latency: 1.88162 ms (enqueue 0.442871 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76643 ms - Host latency: 1.86235 ms (enqueue 0.423657 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76541 ms - Host latency: 1.86116 ms (enqueue 0.428101 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76475 ms - Host latency: 1.8606 ms (enqueue 0.556812 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76543 ms - Host latency: 1.86328 ms (enqueue 0.860791 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76636 ms - Host latency: 1.86392 ms (enqueue 0.852197 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76638 ms - Host latency: 1.86233 ms (enqueue 0.522241 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76592 ms - Host latency: 1.86191 ms (enqueue 0.46582 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76399 ms - Host latency: 1.85986 ms (enqueue 0.966406 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.83965 ms - Host latency: 1.9355 ms (enqueue 0.973608 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76541 ms - Host latency: 1.86125 ms (enqueue 0.925513 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76445 ms - Host latency: 1.86021 ms (enqueue 0.867236 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74458 ms - Host latency: 1.84043 ms (enqueue 0.411572 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7645 ms - Host latency: 1.86038 ms (enqueue 0.416455 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82173 ms - Host latency: 1.9178 ms (enqueue 0.416968 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74619 ms - Host latency: 1.84221 ms (enqueue 0.821851 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76533 ms - Host latency: 1.8613 ms (enqueue 0.94126 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76536 ms - Host latency: 1.86125 ms (enqueue 0.662769 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74531 ms - Host latency: 1.84119 ms (enqueue 0.776929 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.83904 ms - Host latency: 1.93516 ms (enqueue 0.476123 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78687 ms - Host latency: 1.88289 ms (enqueue 0.416992 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74553 ms - Host latency: 1.84141 ms (enqueue 0.465796 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76431 ms - Host latency: 1.86008 ms (enqueue 0.415137 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76514 ms - Host latency: 1.86104 ms (enqueue 0.415942 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.79841 ms - Host latency: 1.89446 ms (enqueue 0.434717 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76511 ms - Host latency: 1.86082 ms (enqueue 0.429736 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76423 ms - Host latency: 1.86006 ms (enqueue 0.419775 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76504 ms - Host latency: 1.86099 ms (enqueue 0.476733 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76636 ms - Host latency: 1.86233 ms (enqueue 0.425049 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78718 ms - Host latency: 1.88306 ms (enqueue 0.545703 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76526 ms - Host latency: 1.86113 ms (enqueue 0.707983 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7656 ms - Host latency: 1.86147 ms (enqueue 0.421265 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74626 ms - Host latency: 1.84211 ms (enqueue 0.430908 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.7989 ms - Host latency: 1.89482 ms (enqueue 0.440649 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76426 ms - Host latency: 1.86016 ms (enqueue 0.624438 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76501 ms - Host latency: 1.86091 ms (enqueue 0.941064 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74553 ms - Host latency: 1.84128 ms (enqueue 0.969067 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81702 ms - Host latency: 1.91313 ms (enqueue 0.91626 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76663 ms - Host latency: 1.86252 ms (enqueue 0.452417 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.78396 ms - Host latency: 1.87979 ms (enqueue 0.42439 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76506 ms - Host latency: 1.86094 ms (enqueue 0.817993 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76426 ms - Host latency: 1.86003 ms (enqueue 0.93457 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74768 ms - Host latency: 1.84453 ms (enqueue 0.764966 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.74607 ms - Host latency: 1.84202 ms (enqueue 0.679126 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76636 ms - Host latency: 1.86223 ms (enqueue 0.606689 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76575 ms - Host latency: 1.8616 ms (enqueue 0.487085 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76477 ms - Host latency: 1.86069 ms (enqueue 0.929297 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.79946 ms - Host latency: 1.89561 ms (enqueue 0.946606 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76702 ms - Host latency: 1.86294 ms (enqueue 0.946143 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76497 ms - Host latency: 1.86082 ms (enqueue 1.78708 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76482 ms - Host latency: 1.86067 ms (enqueue 1.83735 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76465 ms - Host latency: 1.86062 ms (enqueue 1.81335 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.82632 ms - Host latency: 1.92219 ms (enqueue 1.79709 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76523 ms - Host latency: 1.86116 ms (enqueue 1.87974 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.76755 ms - Host latency: 1.86345 ms (enqueue 1.83586 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.77766 ms - Host latency: 1.87354 ms (enqueue 1.82979 ms)\n",
      "[02/05/2024-14:34:23] [I] Average on 10 runs - GPU latency: 1.81753 ms - Host latency: 1.9134 ms (enqueue 1.74895 ms)\n",
      "[02/05/2024-14:34:23] [I] \n",
      "[02/05/2024-14:34:23] [I] === Performance summary ===\n",
      "[02/05/2024-14:34:23] [I] Throughput: 519.015 qps\n",
      "[02/05/2024-14:34:23] [I] Latency: min = 1.83447 ms, max = 3.0827 ms, mean = 1.87145 ms, median = 1.84106 ms, percentile(90%) = 2.02979 ms, percentile(95%) = 2.03821 ms, percentile(99%) = 2.37402 ms\n",
      "[02/05/2024-14:34:23] [I] Enqueue Time: min = 0.365234 ms, max = 2.37109 ms, mean = 0.895383 ms, median = 0.755737 ms, percentile(90%) = 1.84265 ms, percentile(95%) = 1.90643 ms, percentile(99%) = 2.06982 ms\n",
      "[02/05/2024-14:34:23] [I] H2D Latency: min = 0.0922852 ms, max = 0.107666 ms, mean = 0.0927166 ms, median = 0.0926514 ms, percentile(90%) = 0.0927734 ms, percentile(95%) = 0.0928955 ms, percentile(99%) = 0.0930786 ms\n",
      "[02/05/2024-14:34:23] [I] GPU Compute Time: min = 1.73895 ms, max = 2.98688 ms, mean = 1.77542 ms, median = 1.745 ms, percentile(90%) = 1.93335 ms, percentile(95%) = 1.94214 ms, percentile(99%) = 2.2771 ms\n",
      "[02/05/2024-14:34:23] [I] D2H Latency: min = 0.00292969 ms, max = 0.138916 ms, mean = 0.00331688 ms, median = 0.00317383 ms, percentile(90%) = 0.00341797 ms, percentile(95%) = 0.00341797 ms, percentile(99%) = 0.00488281 ms\n",
      "[02/05/2024-14:34:23] [I] Total Host Walltime: 3.00377 s\n",
      "[02/05/2024-14:34:23] [I] Total GPU Compute Time: 2.76788 s\n",
      "[02/05/2024-14:34:23] [W] * GPU compute time is unstable, with coefficient of variance = 5.47881%.\n",
      "[02/05/2024-14:34:23] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[02/05/2024-14:34:23] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/05/2024-14:34:23] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "FP16 = False\n",
    "if FP16:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/05/2024-14:36:10] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "TensorRT 预测结果： [904 490 490 490 904 490 490 490]\n",
      "TensorRT 推理时间： 0.0033461697101593016\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 通过builder创建一个网络\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "\n",
    "# 2. 解析ONNX文件并设置构建配置（需要使用普通未经过优化的onnx模型）\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model_dynamic.onnx\", \"rb\") as model:\n",
    "    parser.parse(model.read())\n",
    "config = builder.create_builder_config()  # 创建构建设置\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 设置内存大小\n",
    "profile = builder.create_optimization_profile()  # 创建优化配置\n",
    "min_shape = (1, 3, 112, 112)  # 设置最小尺寸\n",
    "opt_shape = (4, 3, 224, 224)  # 设置优先尺寸\n",
    "max_shape = (8, 3, 448, 448)  # 设置最大尺寸\n",
    "profile.set_shape(\"input\", min_shape, opt_shape, max_shape)\n",
    "config.add_optimization_profile(profile)  # 添加优化配置\n",
    "\n",
    "# 3. 转换并导出模型\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "# 4. 创建引擎和上下文管理器\n",
    "with open(\"model_dynamic.trt\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 5. 分配输入输出的内存\n",
    "stream = cuda.Stream()\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "output_data = np.empty([raw_data.shape[0], 1000], dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "context.set_input_shape(engine.get_tensor_name(0), input_data.shape)\n",
    "\n",
    "\n",
    "# 6. 推理和计时\n",
    "def predict(input_data):\n",
    "    # 将数据转换到驱动上去\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # 异步执行模型\n",
    "    context.execute_async_v2(bindings, stream.handle, None)  # 同步推理为execute_v2\n",
    "    # 将数据从驱动上转换回来\n",
    "    cuda.memcpy_dtoh_async(output_data, d_output, stream)\n",
    "    # 阻塞调用线程从而同步CUDA流\n",
    "    stream.synchronize()\n",
    "    return output_data\n",
    "\n",
    "\n",
    "dummy = True\n",
    "if dummy:\n",
    "    prediction = predict(input_data)\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    prediction = predict(input_data)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"TensorRT 预测结果：\", predicted_class)\n",
    "print(\"TensorRT 推理时间：\", (end_time - start_time) / num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
