{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1：基准测试 pytorch2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch 推理结果： tensor([904, 490, 490, 904, 904, 490, 904, 490], device='cuda:0')\n",
      "Pytorch 推理时间： 0.00392825984954834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import time\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 导出模型\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# 3. 输入数据\n",
    "raw_data = torch.randn((8, 3, 224, 224), dtype=torch.float32, device=\"cpu\")\n",
    "input_tensor = raw_data.to(\"cuda\")\n",
    "\n",
    "# 4. 进行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    output = model(input_tensor)\n",
    "time_start = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "time_end = time.time()\n",
    "print(\"Pytorch 推理结果：\", predicted_class)\n",
    "print(\"Pytorch 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2：使用 ONNX1.7 优化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 推理结果： [904 490 490 904]\n",
      "ONNX 推理时间： 0.00392825984954834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 转换并导出静态和动态模型\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(8, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model.onnx\",\n",
    ")\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model_dynamic.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输入张量的名称是'input'，仅固定通道维度\n",
    "        \"output\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输出张量的名称是'output'，仅固定通道维度\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. 输入数据\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "\n",
    "# 3. 创建推理会话\n",
    "tensorrt_accelerate = True\n",
    "if tensorrt_accelerate:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    providers = [\n",
    "        \"TensorrtExecutionProvider\",\n",
    "        \"CUDAExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "else:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    sess_options.graph_optimization_level = (\n",
    "        ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    )  # 启用所有适用的图优化技术\n",
    "    providers = [\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "session = ort.InferenceSession(\"model_dynamic.onnx\", sess_options, providers=providers)\n",
    "\n",
    "# 4. 执行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for i in range(num):\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "    predicted_class = np.argmax(output_data[0], axis=1)\n",
    "end_time = time.time()\n",
    "print(\"ONNX 推理结果：\", predicted_class)\n",
    "print(\"ONNX 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3：使用 TensorRT8.6 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02/04/2024-15:26:23] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[02/04/2024-15:26:23] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[02/04/2024-15:26:28] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/04/2024-15:26:58] [W] * GPU compute time is unstable, with coefficient of variance = 11.8316%.\n",
      "[02/04/2024-15:26:58] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[02/04/2024-15:26:23] [I] === Model Options ===\n",
      "[02/04/2024-15:26:23] [I] Format: ONNX\n",
      "[02/04/2024-15:26:23] [I] Model: model_dynamic.onnx\n",
      "[02/04/2024-15:26:23] [I] Output:\n",
      "[02/04/2024-15:26:23] [I] === Build Options ===\n",
      "[02/04/2024-15:26:23] [I] Max batch: explicit batch\n",
      "[02/04/2024-15:26:23] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/04/2024-15:26:23] [I] minTiming: 1\n",
      "[02/04/2024-15:26:23] [I] avgTiming: 8\n",
      "[02/04/2024-15:26:23] [I] Precision: FP32\n",
      "[02/04/2024-15:26:23] [I] LayerPrecisions: \n",
      "[02/04/2024-15:26:23] [I] Layer Device Types: \n",
      "[02/04/2024-15:26:23] [I] Calibration: \n",
      "[02/04/2024-15:26:23] [I] Refit: Disabled\n",
      "[02/04/2024-15:26:23] [I] Version Compatible: Disabled\n",
      "[02/04/2024-15:26:23] [I] TensorRT runtime: full\n",
      "[02/04/2024-15:26:23] [I] Lean DLL Path: \n",
      "[02/04/2024-15:26:23] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/04/2024-15:26:23] [I] Exclude Lean Runtime: Disabled\n",
      "[02/04/2024-15:26:23] [I] Sparsity: Disabled\n",
      "[02/04/2024-15:26:23] [I] Safe mode: Disabled\n",
      "[02/04/2024-15:26:23] [I] Build DLA standalone loadable: Disabled\n",
      "[02/04/2024-15:26:23] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/04/2024-15:26:23] [I] DirectIO mode: Disabled\n",
      "[02/04/2024-15:26:23] [I] Restricted mode: Disabled\n",
      "[02/04/2024-15:26:23] [I] Skip inference: Disabled\n",
      "[02/04/2024-15:26:23] [I] Save engine: model_dynamic.trt\n",
      "[02/04/2024-15:26:23] [I] Load engine: \n",
      "[02/04/2024-15:26:23] [I] Profiling verbosity: 0\n",
      "[02/04/2024-15:26:23] [I] Tactic sources: Using default tactic sources\n",
      "[02/04/2024-15:26:23] [I] timingCacheMode: local\n",
      "[02/04/2024-15:26:23] [I] timingCacheFile: \n",
      "[02/04/2024-15:26:23] [I] Heuristic: Disabled\n",
      "[02/04/2024-15:26:23] [I] Preview Features: Use default preview flags.\n",
      "[02/04/2024-15:26:23] [I] MaxAuxStreams: -1\n",
      "[02/04/2024-15:26:23] [I] BuilderOptimizationLevel: -1\n",
      "[02/04/2024-15:26:23] [I] Input(s)s format: fp32:CHW\n",
      "[02/04/2024-15:26:23] [I] Output(s)s format: fp32:CHW\n",
      "[02/04/2024-15:26:23] [I] Input build shape: input=1x3x112x112+4x3x224x224+8x3x448x448\n",
      "[02/04/2024-15:26:23] [I] Input calibration shapes: model\n",
      "[02/04/2024-15:26:23] [I] === System Options ===\n",
      "[02/04/2024-15:26:23] [I] Device: 0\n",
      "[02/04/2024-15:26:23] [I] DLACore: \n",
      "[02/04/2024-15:26:23] [I] Plugins:\n",
      "[02/04/2024-15:26:23] [I] setPluginsToSerialize:\n",
      "[02/04/2024-15:26:23] [I] dynamicPlugins:\n",
      "[02/04/2024-15:26:23] [I] ignoreParsedPluginLibs: 0\n",
      "[02/04/2024-15:26:23] [I] \n",
      "[02/04/2024-15:26:23] [I] === Inference Options ===\n",
      "[02/04/2024-15:26:23] [I] Batch: Explicit\n",
      "[02/04/2024-15:26:23] [I] Input inference shape: input=4x3x224x224\n",
      "[02/04/2024-15:26:23] [I] Iterations: 10\n",
      "[02/04/2024-15:26:23] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/04/2024-15:26:23] [I] Sleep time: 0ms\n",
      "[02/04/2024-15:26:23] [I] Idle time: 0ms\n",
      "[02/04/2024-15:26:23] [I] Inference Streams: 1\n",
      "[02/04/2024-15:26:23] [I] ExposeDMA: Disabled\n",
      "[02/04/2024-15:26:23] [I] Data transfers: Enabled\n",
      "[02/04/2024-15:26:23] [I] Spin-wait: Disabled\n",
      "[02/04/2024-15:26:23] [I] Multithreading: Disabled\n",
      "[02/04/2024-15:26:23] [I] CUDA Graph: Disabled\n",
      "[02/04/2024-15:26:23] [I] Separate profiling: Disabled\n",
      "[02/04/2024-15:26:23] [I] Time Deserialize: Disabled\n",
      "[02/04/2024-15:26:23] [I] Time Refit: Disabled\n",
      "[02/04/2024-15:26:23] [I] NVTX verbosity: 0\n",
      "[02/04/2024-15:26:23] [I] Persistent Cache Ratio: 0\n",
      "[02/04/2024-15:26:23] [I] Inputs:\n",
      "[02/04/2024-15:26:23] [I] === Reporting Options ===\n",
      "[02/04/2024-15:26:23] [I] Verbose: Disabled\n",
      "[02/04/2024-15:26:23] [I] Averages: 10 inferences\n",
      "[02/04/2024-15:26:23] [I] Percentiles: 90,95,99\n",
      "[02/04/2024-15:26:23] [I] Dump refittable layers:Disabled\n",
      "[02/04/2024-15:26:23] [I] Dump output: Disabled\n",
      "[02/04/2024-15:26:23] [I] Profile: Disabled\n",
      "[02/04/2024-15:26:23] [I] Export timing to JSON file: \n",
      "[02/04/2024-15:26:23] [I] Export output to JSON file: \n",
      "[02/04/2024-15:26:23] [I] Export profile to JSON file: \n",
      "[02/04/2024-15:26:23] [I] \n",
      "[02/04/2024-15:26:23] [I] === Device Information ===\n",
      "[02/04/2024-15:26:23] [I] Selected Device: NVIDIA GeForce RTX 4080\n",
      "[02/04/2024-15:26:23] [I] Compute Capability: 8.9\n",
      "[02/04/2024-15:26:23] [I] SMs: 76\n",
      "[02/04/2024-15:26:23] [I] Device Global Memory: 16375 MiB\n",
      "[02/04/2024-15:26:23] [I] Shared Memory per SM: 100 KiB\n",
      "[02/04/2024-15:26:23] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[02/04/2024-15:26:23] [I] Application Compute Clock Rate: 2.505 GHz\n",
      "[02/04/2024-15:26:23] [I] Application Memory Clock Rate: 11.201 GHz\n",
      "[02/04/2024-15:26:23] [I] \n",
      "[02/04/2024-15:26:23] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/04/2024-15:26:23] [I] \n",
      "[02/04/2024-15:26:23] [I] TensorRT version: 8.6.1\n",
      "[02/04/2024-15:26:23] [I] Loading standard plugins\n",
      "[02/04/2024-15:26:23] [I] [TRT] [MemUsageChange] Init CUDA: CPU -5, GPU +0, now: CPU 25235, GPU 1313 (MiB)\n",
      "[02/04/2024-15:26:28] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1420, GPU +266, now: CPU 27798, GPU 1579 (MiB)\n",
      "[02/04/2024-15:26:28] [I] Start parsing network model.\n",
      "[02/04/2024-15:26:28] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/04/2024-15:26:28] [I] [TRT] Input filename:   model_dynamic.onnx\n",
      "[02/04/2024-15:26:28] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/04/2024-15:26:28] [I] [TRT] Opset version:    17\n",
      "[02/04/2024-15:26:28] [I] [TRT] Producer name:    pytorch\n",
      "[02/04/2024-15:26:28] [I] [TRT] Producer version: 2.1.1\n",
      "[02/04/2024-15:26:28] [I] [TRT] Domain:           \n",
      "[02/04/2024-15:26:28] [I] [TRT] Model version:    0\n",
      "[02/04/2024-15:26:28] [I] [TRT] Doc string:       \n",
      "[02/04/2024-15:26:28] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/04/2024-15:26:28] [I] Finished parsing network model. Parse time: 0.0972608\n",
      "[02/04/2024-15:26:28] [I] [TRT] Graph optimization time: 0.0214123 seconds.\n",
      "[02/04/2024-15:26:28] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/04/2024-15:26:54] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/04/2024-15:26:54] [I] [TRT] Total Host Persistent Memory: 320768\n",
      "[02/04/2024-15:26:54] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[02/04/2024-15:26:54] [I] [TRT] Total Scratch Memory: 38541824\n",
      "[02/04/2024-15:26:54] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 103 MiB, GPU 255 MiB\n",
      "[02/04/2024-15:26:54] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 113 steps to complete.\n",
      "[02/04/2024-15:26:54] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.8802ms to assign 12 blocks to 113 nodes requiring 282608640 bytes.\n",
      "[02/04/2024-15:26:54] [I] [TRT] Total Activation Memory: 282606592\n",
      "[02/04/2024-15:26:54] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +79, GPU +122, now: CPU 79, GPU 122 (MiB)\n",
      "[02/04/2024-15:26:54] [I] Engine built in 31.2689 sec.\n",
      "[02/04/2024-15:26:55] [I] [TRT] Loaded engine size: 124 MiB\n",
      "[02/04/2024-15:26:55] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +121, now: CPU 0, GPU 121 (MiB)\n",
      "[02/04/2024-15:26:55] [I] Engine deserialized in 0.0190433 sec.\n",
      "[02/04/2024-15:26:55] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +270, now: CPU 0, GPU 391 (MiB)\n",
      "[02/04/2024-15:26:55] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/04/2024-15:26:55] [I] Using random values for input input\n",
      "[02/04/2024-15:26:55] [I] Input binding for input with dimensions 4x3x224x224 is created.\n",
      "[02/04/2024-15:26:55] [I] Output binding for output with dimensions 4x1000 is created.\n",
      "[02/04/2024-15:26:55] [I] Starting inference\n",
      "[02/04/2024-15:26:58] [I] Warmup completed 104 queries over 200 ms\n",
      "[02/04/2024-15:26:58] [I] Timing trace has 1550 queries over 3.00439 s\n",
      "[02/04/2024-15:26:58] [I] \n",
      "[02/04/2024-15:26:58] [I] === Trace details ===\n",
      "[02/04/2024-15:26:58] [I] Trace averages of 10 runs:\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.73505 ms - Host latency: 1.83113 ms (enqueue 0.51171 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74418 ms - Host latency: 1.84029 ms (enqueue 0.534529 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89563 ms - Host latency: 1.99178 ms (enqueue 0.508812 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74117 ms - Host latency: 1.83735 ms (enqueue 0.464172 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74356 ms - Host latency: 1.8397 ms (enqueue 0.504074 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.73777 ms - Host latency: 1.83393 ms (enqueue 0.495706 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74154 ms - Host latency: 1.83771 ms (enqueue 0.510495 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7491 ms - Host latency: 1.84541 ms (enqueue 0.5155 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74486 ms - Host latency: 1.84096 ms (enqueue 0.496744 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74222 ms - Host latency: 1.8383 ms (enqueue 0.527444 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74443 ms - Host latency: 1.84057 ms (enqueue 0.503568 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74053 ms - Host latency: 1.83659 ms (enqueue 0.522223 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89684 ms - Host latency: 1.99315 ms (enqueue 0.536008 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74567 ms - Host latency: 1.84189 ms (enqueue 0.550629 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74557 ms - Host latency: 1.84172 ms (enqueue 0.514658 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74501 ms - Host latency: 1.84109 ms (enqueue 0.502887 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74035 ms - Host latency: 1.83648 ms (enqueue 0.505386 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.91177 ms - Host latency: 2.00795 ms (enqueue 0.456982 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74479 ms - Host latency: 1.84086 ms (enqueue 0.502692 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.72352 ms - Host latency: 1.81972 ms (enqueue 0.510767 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74432 ms - Host latency: 1.84044 ms (enqueue 0.527753 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7422 ms - Host latency: 1.83839 ms (enqueue 0.470306 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89866 ms - Host latency: 1.99523 ms (enqueue 0.520435 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74564 ms - Host latency: 1.84168 ms (enqueue 0.478784 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7444 ms - Host latency: 1.84045 ms (enqueue 0.489655 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74462 ms - Host latency: 1.84062 ms (enqueue 0.505017 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.72498 ms - Host latency: 1.82111 ms (enqueue 0.547662 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89344 ms - Host latency: 1.98961 ms (enqueue 0.523615 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74388 ms - Host latency: 1.84001 ms (enqueue 0.487616 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7441 ms - Host latency: 1.84006 ms (enqueue 0.53158 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74513 ms - Host latency: 1.84108 ms (enqueue 0.513184 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89981 ms - Host latency: 1.99596 ms (enqueue 0.521423 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.747 ms - Host latency: 1.84496 ms (enqueue 0.455579 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7437 ms - Host latency: 1.83981 ms (enqueue 0.476099 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7608 ms - Host latency: 1.85698 ms (enqueue 0.480371 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74503 ms - Host latency: 1.84113 ms (enqueue 0.565979 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74977 ms - Host latency: 1.84584 ms (enqueue 0.478149 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74655 ms - Host latency: 1.84272 ms (enqueue 0.490344 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74069 ms - Host latency: 1.83677 ms (enqueue 0.516248 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76054 ms - Host latency: 1.85712 ms (enqueue 0.503931 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74341 ms - Host latency: 1.83951 ms (enqueue 0.519379 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89679 ms - Host latency: 1.99292 ms (enqueue 0.539441 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76166 ms - Host latency: 1.85784 ms (enqueue 0.520972 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7439 ms - Host latency: 1.83993 ms (enqueue 0.514087 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74398 ms - Host latency: 1.83997 ms (enqueue 0.508789 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89725 ms - Host latency: 1.99321 ms (enqueue 0.508899 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74866 ms - Host latency: 1.84479 ms (enqueue 0.55603 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74772 ms - Host latency: 1.84397 ms (enqueue 0.499377 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.73156 ms - Host latency: 1.82748 ms (enqueue 0.501489 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.81323 ms - Host latency: 1.91062 ms (enqueue 0.477454 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89982 ms - Host latency: 1.99633 ms (enqueue 0.517078 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74557 ms - Host latency: 1.84169 ms (enqueue 0.51051 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74535 ms - Host latency: 1.84269 ms (enqueue 0.598669 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.75153 ms - Host latency: 1.84752 ms (enqueue 0.502747 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7449 ms - Host latency: 1.84095 ms (enqueue 0.514331 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.90601 ms - Host latency: 2.00302 ms (enqueue 0.520715 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7495 ms - Host latency: 1.84552 ms (enqueue 0.477124 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74609 ms - Host latency: 1.84214 ms (enqueue 0.497327 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74375 ms - Host latency: 1.83979 ms (enqueue 0.496606 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7442 ms - Host latency: 1.84043 ms (enqueue 0.499158 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74785 ms - Host latency: 1.84402 ms (enqueue 0.514636 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74185 ms - Host latency: 1.83806 ms (enqueue 0.485156 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.75647 ms - Host latency: 1.85259 ms (enqueue 0.515234 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74377 ms - Host latency: 1.83981 ms (enqueue 0.501709 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74685 ms - Host latency: 1.8429 ms (enqueue 0.519116 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89934 ms - Host latency: 1.99569 ms (enqueue 0.49751 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74453 ms - Host latency: 1.84082 ms (enqueue 0.496118 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76492 ms - Host latency: 1.86102 ms (enqueue 0.517029 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74475 ms - Host latency: 1.84076 ms (enqueue 0.499231 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74589 ms - Host latency: 1.84197 ms (enqueue 0.502783 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89855 ms - Host latency: 1.99482 ms (enqueue 0.507703 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74414 ms - Host latency: 1.84027 ms (enqueue 0.465149 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74312 ms - Host latency: 1.8392 ms (enqueue 0.493799 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74468 ms - Host latency: 1.8416 ms (enqueue 0.504919 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.72434 ms - Host latency: 1.82257 ms (enqueue 0.513916 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.8918 ms - Host latency: 1.98801 ms (enqueue 0.560596 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74481 ms - Host latency: 1.841 ms (enqueue 0.521899 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74341 ms - Host latency: 1.84047 ms (enqueue 0.513489 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74451 ms - Host latency: 1.8438 ms (enqueue 0.528601 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89557 ms - Host latency: 1.9926 ms (enqueue 0.509778 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.77051 ms - Host latency: 1.86669 ms (enqueue 0.571338 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74099 ms - Host latency: 1.83713 ms (enqueue 0.503259 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74496 ms - Host latency: 1.84108 ms (enqueue 0.493555 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74421 ms - Host latency: 1.84031 ms (enqueue 0.494666 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.90803 ms - Host latency: 2.00427 ms (enqueue 0.52356 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74464 ms - Host latency: 1.84327 ms (enqueue 0.459045 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76003 ms - Host latency: 1.85605 ms (enqueue 0.506287 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74358 ms - Host latency: 1.83997 ms (enqueue 0.525195 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74678 ms - Host latency: 1.84279 ms (enqueue 0.530994 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89746 ms - Host latency: 1.9948 ms (enqueue 0.516931 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76991 ms - Host latency: 1.86608 ms (enqueue 0.487109 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74799 ms - Host latency: 1.84412 ms (enqueue 0.502747 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74375 ms - Host latency: 1.83979 ms (enqueue 0.511462 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76384 ms - Host latency: 1.85985 ms (enqueue 0.513416 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89178 ms - Host latency: 1.98801 ms (enqueue 0.520276 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7467 ms - Host latency: 1.84294 ms (enqueue 0.488098 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74268 ms - Host latency: 1.83872 ms (enqueue 0.511914 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74353 ms - Host latency: 1.83955 ms (enqueue 0.52041 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74285 ms - Host latency: 1.83887 ms (enqueue 0.497192 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.90254 ms - Host latency: 1.99888 ms (enqueue 0.587231 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74419 ms - Host latency: 1.84021 ms (enqueue 0.531934 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74299 ms - Host latency: 1.83901 ms (enqueue 0.529834 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74583 ms - Host latency: 1.84182 ms (enqueue 0.501709 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89302 ms - Host latency: 1.98921 ms (enqueue 0.511353 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74688 ms - Host latency: 1.84302 ms (enqueue 0.485864 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74177 ms - Host latency: 1.83794 ms (enqueue 0.480908 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76082 ms - Host latency: 1.85791 ms (enqueue 0.536108 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.72505 ms - Host latency: 1.82114 ms (enqueue 0.502588 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89944 ms - Host latency: 1.99575 ms (enqueue 0.499219 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.75327 ms - Host latency: 1.84939 ms (enqueue 0.439844 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74165 ms - Host latency: 1.83765 ms (enqueue 0.505054 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.73794 ms - Host latency: 1.83828 ms (enqueue 0.512231 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74419 ms - Host latency: 1.84268 ms (enqueue 0.534155 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89011 ms - Host latency: 1.98625 ms (enqueue 0.507788 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74741 ms - Host latency: 1.84468 ms (enqueue 0.463428 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74858 ms - Host latency: 1.84617 ms (enqueue 0.469287 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.77498 ms - Host latency: 1.87136 ms (enqueue 0.474609 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74485 ms - Host latency: 1.84192 ms (enqueue 0.534131 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.91826 ms - Host latency: 2.01489 ms (enqueue 0.54519 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.78979 ms - Host latency: 1.88706 ms (enqueue 0.51228 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.72366 ms - Host latency: 1.82393 ms (enqueue 0.493311 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76274 ms - Host latency: 1.85999 ms (enqueue 0.494946 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74512 ms - Host latency: 1.84119 ms (enqueue 0.503223 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.901 ms - Host latency: 1.99841 ms (enqueue 0.517554 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74534 ms - Host latency: 1.84167 ms (enqueue 0.448193 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74434 ms - Host latency: 1.84038 ms (enqueue 0.525586 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76428 ms - Host latency: 1.86033 ms (enqueue 0.496655 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7415 ms - Host latency: 1.8375 ms (enqueue 0.501001 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.91318 ms - Host latency: 2.00942 ms (enqueue 0.520557 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76213 ms - Host latency: 1.85833 ms (enqueue 0.481152 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74585 ms - Host latency: 1.84192 ms (enqueue 0.761206 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76064 ms - Host latency: 1.85669 ms (enqueue 0.50813 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74663 ms - Host latency: 1.84255 ms (enqueue 0.497656 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89714 ms - Host latency: 1.99326 ms (enqueue 0.594678 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74302 ms - Host latency: 1.83923 ms (enqueue 0.419434 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74397 ms - Host latency: 1.83999 ms (enqueue 0.47417 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76355 ms - Host latency: 1.85994 ms (enqueue 0.492749 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76531 ms - Host latency: 1.86143 ms (enqueue 0.490649 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.89941 ms - Host latency: 1.99626 ms (enqueue 0.477563 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74353 ms - Host latency: 1.83975 ms (enqueue 0.482202 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7447 ms - Host latency: 1.84075 ms (enqueue 0.475562 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74204 ms - Host latency: 1.83804 ms (enqueue 0.504932 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.93555 ms - Host latency: 2.03167 ms (enqueue 0.521704 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76958 ms - Host latency: 1.86575 ms (enqueue 0.471973 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.75955 ms - Host latency: 1.85557 ms (enqueue 0.600903 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74688 ms - Host latency: 1.84287 ms (enqueue 0.492651 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74392 ms - Host latency: 1.83987 ms (enqueue 0.508081 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.91907 ms - Host latency: 2.01536 ms (enqueue 0.47749 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74097 ms - Host latency: 1.83704 ms (enqueue 0.565308 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76899 ms - Host latency: 1.86504 ms (enqueue 0.491968 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.75959 ms - Host latency: 1.85566 ms (enqueue 0.492188 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.7321 ms - Host latency: 1.82825 ms (enqueue 0.500366 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.76875 ms - Host latency: 1.86492 ms (enqueue 0.508545 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74592 ms - Host latency: 1.84338 ms (enqueue 0.454736 ms)\n",
      "[02/04/2024-15:26:58] [I] Average on 10 runs - GPU latency: 1.74597 ms - Host latency: 1.84189 ms (enqueue 0.488257 ms)\n",
      "[02/04/2024-15:26:58] [I] \n",
      "[02/04/2024-15:26:58] [I] === Performance summary ===\n",
      "[02/04/2024-15:26:58] [I] Throughput: 515.911 qps\n",
      "[02/04/2024-15:26:58] [I] Latency: min = 1.81316 ms, max = 3.56128 ms, mean = 1.87198 ms, median = 1.82068 ms, percentile(90%) = 2.01099 ms, percentile(95%) = 2.03479 ms, percentile(99%) = 3.32715 ms\n",
      "[02/04/2024-15:26:58] [I] Enqueue Time: min = 0.370239 ms, max = 1.9353 ms, mean = 0.508289 ms, median = 0.506897 ms, percentile(90%) = 0.557251 ms, percentile(95%) = 0.589111 ms, percentile(99%) = 0.721069 ms\n",
      "[02/04/2024-15:26:58] [I] H2D Latency: min = 0.0924072 ms, max = 0.112549 ms, mean = 0.0929666 ms, median = 0.0927734 ms, percentile(90%) = 0.0930176 ms, percentile(95%) = 0.0932617 ms, percentile(99%) = 0.0952148 ms\n",
      "[02/04/2024-15:26:58] [I] GPU Compute Time: min = 1.71721 ms, max = 3.46411 ms, mean = 1.7756 ms, median = 1.72443 ms, percentile(90%) = 1.91479 ms, percentile(95%) = 1.93848 ms, percentile(99%) = 3.23071 ms\n",
      "[02/04/2024-15:26:58] [I] D2H Latency: min = 0.00292969 ms, max = 0.0274658 ms, mean = 0.00340764 ms, median = 0.00317383 ms, percentile(90%) = 0.00341797 ms, percentile(95%) = 0.00360107 ms, percentile(99%) = 0.0124512 ms\n",
      "[02/04/2024-15:26:58] [I] Total Host Walltime: 3.00439 s\n",
      "[02/04/2024-15:26:58] [I] Total GPU Compute Time: 2.75219 s\n",
      "[02/04/2024-15:26:58] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/04/2024-15:26:58] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "FP16 = False\n",
    "if FP16:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 预测结果： [904 490 490 904 904 490 904 490]\n",
      "TensorRT 推理时间： 0.002949941158294678\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 通过builder创建一个网络\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "\n",
    "# 2. 解析ONNX文件并设置构建配置（需要使用普通未经过优化的onnx模型）\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model.onnx\", \"rb\") as model:\n",
    "    parser.parse(model.read())\n",
    "config = builder.create_builder_config()  # 创建构建设置\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 设置内存大小\n",
    "profile = builder.create_optimization_profile()  # 创建优化配置\n",
    "min_shape = (1, 3, 112, 112)  # 设置最小尺寸\n",
    "opt_shape = (4, 3, 224, 224)  # 设置优先尺寸\n",
    "max_shape = (8, 3, 448, 448)  # 设置最大尺寸\n",
    "profile.set_shape(\"input\", min_shape, opt_shape, max_shape)\n",
    "config.add_optimization_profile(profile)  # 添加优化配置\n",
    "\n",
    "# 3. 转换并导出模型\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "# 4. 创建引擎和上下文管理器\n",
    "with open(\"model.trt\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 5. 分配输入输出的内存\n",
    "stream = cuda.Stream()\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "output_data = np.empty([raw_data.shape[0], 1000], dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "context.set_input_shape(\"input\", input_data.shape)\n",
    "\n",
    "# 6. 推理和计时\n",
    "def predict(input_data):\n",
    "    # 将数据转换到驱动上去\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # 异步执行模型\n",
    "    context.execute_async_v2(bindings, stream.handle, None)  # 同步推理为execute_v2\n",
    "    # 将数据从驱动上转换回来\n",
    "    cuda.memcpy_dtoh_async(output_data, d_output, stream)\n",
    "    # 阻塞调用线程从而同步CUDA流\n",
    "    stream.synchronize()\n",
    "    return output_data\n",
    "\n",
    "dummy = True\n",
    "if dummy:\n",
    "    prediction = predict(input_data)\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    prediction = predict(input_data)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"TensorRT 预测结果：\", predicted_class)\n",
    "print(\"TensorRT 推理时间：\", (end_time - start_time) / num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
