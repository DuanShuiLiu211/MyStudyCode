{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1：基准测试 pytorch2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch 推理结果： tensor([490, 490, 490, 490, 490, 490, 490, 490], device='cuda:0')\n",
      "Pytorch 推理时间： 0.004344187259674072\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import time\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 导出模型\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# 3. 输入数据\n",
    "raw_data = torch.randn((8, 3, 224, 224), dtype=torch.float32, device=\"cpu\")\n",
    "input_tensor = raw_data.to(\"cuda\")\n",
    "\n",
    "# 4. 进行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    output = model(input_tensor)\n",
    "time_start = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "time_end = time.time()\n",
    "print(\"Pytorch 推理结果：\", predicted_class)\n",
    "print(\"Pytorch 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2：使用 ONNX1.7 优化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 推理结果： [490 490 490 490 490 490 490 490]\n",
      "ONNX 推理时间： 0.004344187259674072\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 转换并导出静态和动态模型\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model.onnx\",\n",
    ")\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model_dynamic.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输入张量的名称是'input'，仅固定通道维度\n",
    "        \"output\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输出张量的名称是'output'，仅固定通道维度\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. 输入数据\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "\n",
    "# 3. 创建推理会话\n",
    "tensorrt_accelerate = False\n",
    "if tensorrt_accelerate:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    providers = [\n",
    "        \"TensorrtExecutionProvider\",\n",
    "        \"CUDAExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "else:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    sess_options.graph_optimization_level = (\n",
    "        ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    )  # 启用所有适用的图优化技术\n",
    "    providers = [\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "session = ort.InferenceSession(\"model_dynamic.onnx\", sess_options, providers=providers)\n",
    "\n",
    "# 4. 执行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for i in range(num):\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "    predicted_class = np.argmax(output_data[0], axis=1)\n",
    "end_time = time.time()\n",
    "print(\"ONNX 推理结果：\", predicted_class)\n",
    "print(\"ONNX 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3：使用 TensorRT8.6 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch\n",
      "[02/05/2024-14:50:46] [I] === Model Options ===\n",
      "[02/05/2024-14:50:46] [I] Format: ONNX\n",
      "[02/05/2024-14:50:46] [I] Model: model_dynamic.onnx\n",
      "[02/05/2024-14:50:46] [I] Output:\n",
      "[02/05/2024-14:50:46] [I] === Build Options ===\n",
      "[02/05/2024-14:50:46] [I] Max batch: explicit batch\n",
      "[02/05/2024-14:50:46] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/05/2024-14:50:46] [I] minTiming: 1\n",
      "[02/05/2024-14:50:46] [I] avgTiming: 8\n",
      "[02/05/2024-14:50:46] [I] Precision: FP32\n",
      "[02/05/2024-14:50:46] [I] LayerPrecisions: \n",
      "[02/05/2024-14:50:46] [I] Layer Device Types: \n",
      "[02/05/2024-14:50:46] [I] Calibration: \n",
      "[02/05/2024-14:50:46] [I] Refit: Disabled\n",
      "[02/05/2024-14:50:46] [I] Version Compatible: Disabled\n",
      "[02/05/2024-14:50:46] [I] TensorRT runtime: full\n",
      "[02/05/2024-14:50:46] [I] Lean DLL Path: \n",
      "[02/05/2024-14:50:46] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/05/2024-14:50:46] [I] Exclude Lean Runtime: Disabled\n",
      "[02/05/2024-14:50:46] [I] Sparsity: Disabled\n",
      "[02/05/2024-14:50:46] [I] Safe mode: Disabled\n",
      "[02/05/2024-14:50:46] [I] Build DLA standalone loadable: Disabled\n",
      "[02/05/2024-14:50:46] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/05/2024-14:50:46] [I] DirectIO mode: Disabled\n",
      "[02/05/2024-14:50:46] [I] Restricted mode: Disabled\n",
      "[02/05/2024-14:50:46] [I] Skip inference: Disabled\n",
      "[02/05/2024-14:50:46] [I] Save engine: model_dynamic.trt\n",
      "[02/05/2024-14:50:46] [I] Load engine: \n",
      "[02/05/2024-14:50:46] [I] Profiling verbosity: 0\n",
      "[02/05/2024-14:50:46] [I] Tactic sources: Using default tactic sources\n",
      "[02/05/2024-14:50:46] [I] timingCacheMode: local\n",
      "[02/05/2024-14:50:46] [I] timingCacheFile: \n",
      "[02/05/2024-14:50:46] [I] Heuristic: Disabled\n",
      "[02/05/2024-14:50:46] [I] Preview Features: Use default preview flags.\n",
      "[02/05/2024-14:50:46] [I] MaxAuxStreams: -1\n",
      "[02/05/2024-14:50:46] [I] BuilderOptimizationLevel: -1\n",
      "[02/05/2024-14:50:46] [I] Input(s)s format: fp32:CHW\n",
      "[02/05/2024-14:50:46] [I] Output(s)s format: fp32:CHW\n",
      "[02/05/2024-14:50:46] [I] Input build shape: input=1x3x112x112+4x3x224x224+8x3x448x448\n",
      "[02/05/2024-14:50:46] [I] Input calibration shapes: model\n",
      "[02/05/2024-14:50:46] [I] === System Options ===\n",
      "[02/05/2024-14:50:46] [I] Device: 0\n",
      "[02/05/2024-14:50:46] [I] DLACore: \n",
      "[02/05/2024-14:50:46] [I] Plugins:\n",
      "[02/05/2024-14:50:46] [I] setPluginsToSerialize:\n",
      "[02/05/2024-14:50:46] [I] dynamicPlugins:\n",
      "[02/05/2024-14:50:46] [I] ignoreParsedPluginLibs: 0\n",
      "[02/05/2024-14:50:46] [I] \n",
      "[02/05/2024-14:50:46] [I] === Inference Options ===\n",
      "[02/05/2024-14:50:46] [I] Batch: Explicit\n",
      "[02/05/2024-14:50:46] [I] Input inference shape: input=4x3x224x224\n",
      "[02/05/2024-14:50:46] [I] Iterations: 10\n",
      "[02/05/2024-14:50:46] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/05/2024-14:50:46] [I] Sleep time: 0ms\n",
      "[02/05/2024-14:50:46] [I] Idle time: 0ms\n",
      "[02/05/2024-14:50:46] [I] Inference Streams: 1\n",
      "[02/05/2024-14:50:46] [I] ExposeDMA: Disabled\n",
      "[02/05/2024-14:50:46] [I] Data transfers: Enabled\n",
      "[02/05/2024-14:50:46] [I] Spin-wait: Disabled\n",
      "[02/05/2024-14:50:46] [I] Multithreading: Disabled\n",
      "[02/05/2024-14:50:46] [I] CUDA Graph: Disabled\n",
      "[02/05/2024-14:50:46] [I] Separate profiling: Disabled\n",
      "[02/05/2024-14:50:46] [I] Time Deserialize: Disabled\n",
      "[02/05/2024-14:50:46] [I] Time Refit: Disabled\n",
      "[02/05/2024-14:50:46] [I] NVTX verbosity: 0\n",
      "[02/05/2024-14:50:46] [I] Persistent Cache Ratio: 0\n",
      "[02/05/2024-14:50:46] [I] Inputs:\n",
      "[02/05/2024-14:50:46] [I] === Reporting Options ===\n",
      "[02/05/2024-14:50:46] [I] Verbose: Disabled\n",
      "[02/05/2024-14:50:46] [I] Averages: 10 inferences\n",
      "[02/05/2024-14:50:46] [I] Percentiles: 90,95,99\n",
      "[02/05/2024-14:50:46] [I] Dump refittable layers:Disabled\n",
      "[02/05/2024-14:50:46] [I] Dump output: Disabled\n",
      "[02/05/2024-14:50:46] [I] Profile: Disabled\n",
      "[02/05/2024-14:50:46] [I] Export timing to JSON file: \n",
      "[02/05/2024-14:50:46] [I] Export output to JSON file: \n",
      "[02/05/2024-14:50:46] [I] Export profile to JSON file: \n",
      "[02/05/2024-14:50:46] [I] \n",
      "[02/05/2024-14:50:46] [I] === Device Information ===\n",
      "[02/05/2024-14:50:46] [I] Selected Device: NVIDIA GeForce RTX 4080\n",
      "[02/05/2024-14:50:46] [I] Compute Capability: 8.9\n",
      "[02/05/2024-14:50:46] [I] SMs: 76\n",
      "[02/05/2024-14:50:46] [I] Device Global Memory: 16375 MiB\n",
      "[02/05/2024-14:50:46] [I] Shared Memory per SM: 100 KiB\n",
      "[02/05/2024-14:50:46] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[02/05/2024-14:50:46] [I] Application Compute Clock Rate: 2.505 GHz\n",
      "[02/05/2024-14:50:46] [I] Application Memory Clock Rate: 11.201 GHz\n",
      "[02/05/2024-14:50:46] [I] \n",
      "[02/05/2024-14:50:46] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/05/2024-14:50:46] [I] \n",
      "[02/05/2024-14:50:46] [I] TensorRT version: 8.6.1\n",
      "[02/05/2024-14:50:46] [I] Loading standard plugins\n",
      "[02/05/2024-14:50:46] [I] [TRT] [MemUsageChange] Init CUDA: CPU +8, GPU +0, now: CPU 14536, GPU 1313 (MiB)\n",
      "[02/05/2024-14:50:50] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1455, GPU +266, now: CPU 17124, GPU 1579 (MiB)\n",
      "[02/05/2024-14:50:50] [I] Start parsing network model.\n",
      "[02/05/2024-14:50:50] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/05/2024-14:50:50] [I] [TRT] Input filename:   model_dynamic.onnx\n",
      "[02/05/2024-14:50:50] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/05/2024-14:50:50] [I] [TRT] Opset version:    17\n",
      "[02/05/2024-14:50:50] [I] [TRT] Producer name:    pytorch\n",
      "[02/05/2024-14:50:50] [I] [TRT] Producer version: 2.2.0\n",
      "[02/05/2024-14:50:50] [I] [TRT] Domain:           \n",
      "[02/05/2024-14:50:50] [I] [TRT] Model version:    0\n",
      "[02/05/2024-14:50:50] [I] [TRT] Doc string:       \n",
      "[02/05/2024-14:50:50] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/05/2024-14:50:50] [I] Finished parsing network model. Parse time: 0.0797396\n",
      "[02/05/2024-14:50:50] [I] [TRT] Graph optimization time: 0.0216844 seconds.\n",
      "[02/05/2024-14:50:50] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/05/2024-14:51:13] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/05/2024-14:51:14] [I] [TRT] Total Host Persistent Memory: 320640\n",
      "[02/05/2024-14:51:14] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[02/05/2024-14:51:14] [I] [TRT] Total Scratch Memory: 38541824\n",
      "[02/05/2024-14:51:14] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 103 MiB, GPU 255 MiB\n",
      "[02/05/2024-14:51:14] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 117 steps to complete.\n",
      "[02/05/2024-14:51:14] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.1507ms to assign 13 blocks to 117 nodes requiring 282609664 bytes.\n",
      "[02/05/2024-14:51:14] [I] [TRT] Total Activation Memory: 282607616\n",
      "[02/05/2024-14:51:14] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +79, GPU +122, now: CPU 79, GPU 122 (MiB)\n",
      "[02/05/2024-14:51:14] [I] Engine built in 27.8297 sec.\n",
      "[02/05/2024-14:51:14] [I] [TRT] Loaded engine size: 124 MiB\n",
      "[02/05/2024-14:51:14] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +121, now: CPU 0, GPU 121 (MiB)\n",
      "[02/05/2024-14:51:14] [I] Engine deserialized in 0.0182985 sec.\n",
      "[02/05/2024-14:51:14] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +270, now: CPU 0, GPU 391 (MiB)\n",
      "[02/05/2024-14:51:14] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/05/2024-14:51:14] [I] Using random values for input input\n",
      "[02/05/2024-14:51:14] [I] Input binding for input with dimensions 4x3x224x224 is created.\n",
      "[02/05/2024-14:51:14] [I] Output binding for output with dimensions 4x1000 is created.\n",
      "[02/05/2024-14:51:14] [I] Starting inference\n",
      "[02/05/2024-14:51:17] [I] Warmup completed 105 queries over 200 ms\n",
      "[02/05/2024-14:51:17] [I] Timing trace has 1563 queries over 3.00285 s\n",
      "[02/05/2024-14:51:17] [I] \n",
      "[02/05/2024-14:51:17] [I] === Trace details ===\n",
      "[02/05/2024-14:51:17] [I] Trace averages of 10 runs:\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76264 ms - Host latency: 1.85971 ms (enqueue 0.514412 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.79565 ms - Host latency: 1.89173 ms (enqueue 0.566988 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.80369 ms - Host latency: 1.89971 ms (enqueue 0.53941 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7438 ms - Host latency: 1.83973 ms (enqueue 0.545135 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76182 ms - Host latency: 1.85778 ms (enqueue 0.538275 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81487 ms - Host latency: 1.91098 ms (enqueue 0.528278 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76169 ms - Host latency: 1.85758 ms (enqueue 0.537149 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76339 ms - Host latency: 1.85937 ms (enqueue 0.529062 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76237 ms - Host latency: 1.85829 ms (enqueue 0.537839 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76192 ms - Host latency: 1.85788 ms (enqueue 0.526199 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81558 ms - Host latency: 1.91165 ms (enqueue 0.508765 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.75998 ms - Host latency: 1.85591 ms (enqueue 0.540799 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.78122 ms - Host latency: 1.87714 ms (enqueue 0.548987 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76248 ms - Host latency: 1.85935 ms (enqueue 0.53284 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7626 ms - Host latency: 1.85847 ms (enqueue 0.522552 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76103 ms - Host latency: 1.85699 ms (enqueue 0.522382 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.83902 ms - Host latency: 1.93523 ms (enqueue 0.531699 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76171 ms - Host latency: 1.85771 ms (enqueue 0.531311 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76266 ms - Host latency: 1.85854 ms (enqueue 0.537677 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.75951 ms - Host latency: 1.85544 ms (enqueue 0.534369 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81437 ms - Host latency: 1.91086 ms (enqueue 0.541522 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76502 ms - Host latency: 1.86094 ms (enqueue 0.552112 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76259 ms - Host latency: 1.85845 ms (enqueue 0.53205 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76124 ms - Host latency: 1.85714 ms (enqueue 0.542621 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81487 ms - Host latency: 1.91089 ms (enqueue 0.561163 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76472 ms - Host latency: 1.86064 ms (enqueue 0.552545 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76293 ms - Host latency: 1.85885 ms (enqueue 0.525842 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.78185 ms - Host latency: 1.8778 ms (enqueue 0.528729 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.80447 ms - Host latency: 1.90164 ms (enqueue 0.553052 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76687 ms - Host latency: 1.86279 ms (enqueue 0.472876 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76242 ms - Host latency: 1.85831 ms (enqueue 0.532843 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74189 ms - Host latency: 1.83786 ms (enqueue 0.501862 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76164 ms - Host latency: 1.85764 ms (enqueue 0.537415 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81789 ms - Host latency: 1.91395 ms (enqueue 0.621979 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76481 ms - Host latency: 1.86079 ms (enqueue 0.542133 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7639 ms - Host latency: 1.85994 ms (enqueue 0.543463 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76366 ms - Host latency: 1.85963 ms (enqueue 0.544934 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76342 ms - Host latency: 1.8594 ms (enqueue 0.526123 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81996 ms - Host latency: 1.916 ms (enqueue 0.46983 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76299 ms - Host latency: 1.86002 ms (enqueue 0.530096 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76273 ms - Host latency: 1.85867 ms (enqueue 0.493262 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.78243 ms - Host latency: 1.87839 ms (enqueue 0.521594 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74299 ms - Host latency: 1.83903 ms (enqueue 0.547247 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.77958 ms - Host latency: 1.8757 ms (enqueue 0.558618 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76119 ms - Host latency: 1.85715 ms (enqueue 0.526392 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76243 ms - Host latency: 1.85835 ms (enqueue 0.543738 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76211 ms - Host latency: 1.85803 ms (enqueue 0.541394 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81425 ms - Host latency: 1.91019 ms (enqueue 0.554443 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76383 ms - Host latency: 1.85973 ms (enqueue 0.502783 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7624 ms - Host latency: 1.8583 ms (enqueue 0.533496 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7615 ms - Host latency: 1.85741 ms (enqueue 0.538916 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.83191 ms - Host latency: 1.92891 ms (enqueue 0.548242 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76239 ms - Host latency: 1.8583 ms (enqueue 0.528967 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76216 ms - Host latency: 1.85802 ms (enqueue 0.531165 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76277 ms - Host latency: 1.85863 ms (enqueue 0.551624 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74115 ms - Host latency: 1.83712 ms (enqueue 0.538782 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81569 ms - Host latency: 1.91311 ms (enqueue 0.526074 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76487 ms - Host latency: 1.86079 ms (enqueue 0.544568 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76453 ms - Host latency: 1.86042 ms (enqueue 0.542786 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7626 ms - Host latency: 1.85851 ms (enqueue 0.536475 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76173 ms - Host latency: 1.85758 ms (enqueue 0.536853 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.8168 ms - Host latency: 1.9129 ms (enqueue 0.570227 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76375 ms - Host latency: 1.85973 ms (enqueue 0.531531 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.763 ms - Host latency: 1.85891 ms (enqueue 0.549084 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76218 ms - Host latency: 1.8582 ms (enqueue 0.529065 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81406 ms - Host latency: 1.91006 ms (enqueue 0.547571 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7636 ms - Host latency: 1.8595 ms (enqueue 0.548694 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76167 ms - Host latency: 1.85759 ms (enqueue 0.512683 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76216 ms - Host latency: 1.85813 ms (enqueue 0.525232 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76368 ms - Host latency: 1.85963 ms (enqueue 0.536975 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81472 ms - Host latency: 1.91075 ms (enqueue 0.578113 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.78604 ms - Host latency: 1.88196 ms (enqueue 0.520789 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.75883 ms - Host latency: 1.85475 ms (enqueue 0.526318 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76141 ms - Host latency: 1.85736 ms (enqueue 0.536353 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76188 ms - Host latency: 1.85781 ms (enqueue 0.52793 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.79496 ms - Host latency: 1.89097 ms (enqueue 0.55824 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76527 ms - Host latency: 1.86121 ms (enqueue 0.526379 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76167 ms - Host latency: 1.85759 ms (enqueue 0.559851 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76201 ms - Host latency: 1.85795 ms (enqueue 0.539587 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7411 ms - Host latency: 1.83705 ms (enqueue 0.535388 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76487 ms - Host latency: 1.86185 ms (enqueue 0.514124 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76376 ms - Host latency: 1.85966 ms (enqueue 0.533386 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76178 ms - Host latency: 1.85773 ms (enqueue 0.531934 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76274 ms - Host latency: 1.85867 ms (enqueue 0.526135 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74109 ms - Host latency: 1.83709 ms (enqueue 0.533789 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.84287 ms - Host latency: 1.93904 ms (enqueue 0.541284 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76318 ms - Host latency: 1.85935 ms (enqueue 0.526599 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76249 ms - Host latency: 1.85844 ms (enqueue 0.535193 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76221 ms - Host latency: 1.85812 ms (enqueue 0.520374 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76161 ms - Host latency: 1.85753 ms (enqueue 0.546741 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81345 ms - Host latency: 1.90961 ms (enqueue 0.537463 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7656 ms - Host latency: 1.86154 ms (enqueue 0.53031 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76338 ms - Host latency: 1.85931 ms (enqueue 0.584033 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76122 ms - Host latency: 1.85808 ms (enqueue 0.539001 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76113 ms - Host latency: 1.85697 ms (enqueue 0.537732 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81254 ms - Host latency: 1.90883 ms (enqueue 0.533582 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76279 ms - Host latency: 1.85869 ms (enqueue 0.543579 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74368 ms - Host latency: 1.83967 ms (enqueue 0.530933 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76206 ms - Host latency: 1.85803 ms (enqueue 0.548633 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76211 ms - Host latency: 1.85811 ms (enqueue 0.530225 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.83594 ms - Host latency: 1.93188 ms (enqueue 0.529956 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76365 ms - Host latency: 1.85947 ms (enqueue 0.539722 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76201 ms - Host latency: 1.85798 ms (enqueue 0.528491 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74187 ms - Host latency: 1.8377 ms (enqueue 0.537231 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76201 ms - Host latency: 1.85806 ms (enqueue 0.533374 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81624 ms - Host latency: 1.91223 ms (enqueue 0.556079 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76394 ms - Host latency: 1.85974 ms (enqueue 0.539624 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76277 ms - Host latency: 1.86128 ms (enqueue 0.527881 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76125 ms - Host latency: 1.8572 ms (enqueue 0.545898 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.75688 ms - Host latency: 1.85271 ms (enqueue 0.542944 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81685 ms - Host latency: 1.91296 ms (enqueue 0.530127 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76414 ms - Host latency: 1.86089 ms (enqueue 0.525635 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74214 ms - Host latency: 1.83818 ms (enqueue 0.54624 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76162 ms - Host latency: 1.85759 ms (enqueue 0.532104 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81475 ms - Host latency: 1.91082 ms (enqueue 0.522436 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76421 ms - Host latency: 1.86016 ms (enqueue 0.518286 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7822 ms - Host latency: 1.87817 ms (enqueue 0.544141 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7626 ms - Host latency: 1.85925 ms (enqueue 0.539355 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81519 ms - Host latency: 1.91133 ms (enqueue 0.52998 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76462 ms - Host latency: 1.86057 ms (enqueue 0.586377 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76499 ms - Host latency: 1.86101 ms (enqueue 0.537842 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76282 ms - Host latency: 1.85881 ms (enqueue 0.536401 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7427 ms - Host latency: 1.83862 ms (enqueue 0.572729 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.8187 ms - Host latency: 1.91484 ms (enqueue 0.531641 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76326 ms - Host latency: 1.85918 ms (enqueue 0.523755 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76135 ms - Host latency: 1.85723 ms (enqueue 0.539746 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76238 ms - Host latency: 1.85818 ms (enqueue 0.534497 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81487 ms - Host latency: 1.91096 ms (enqueue 0.550391 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7656 ms - Host latency: 1.8616 ms (enqueue 0.519189 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7625 ms - Host latency: 1.85852 ms (enqueue 0.527515 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.77883 ms - Host latency: 1.87476 ms (enqueue 0.532422 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76165 ms - Host latency: 1.85759 ms (enqueue 0.565454 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76746 ms - Host latency: 1.8645 ms (enqueue 0.531543 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76228 ms - Host latency: 1.8583 ms (enqueue 0.541602 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76223 ms - Host latency: 1.85808 ms (enqueue 0.519897 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7616 ms - Host latency: 1.85759 ms (enqueue 0.549219 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81619 ms - Host latency: 1.91223 ms (enqueue 0.566016 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76492 ms - Host latency: 1.86086 ms (enqueue 0.540625 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7636 ms - Host latency: 1.85964 ms (enqueue 0.518433 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76208 ms - Host latency: 1.85801 ms (enqueue 0.511719 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81428 ms - Host latency: 1.91028 ms (enqueue 0.506567 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76467 ms - Host latency: 1.86045 ms (enqueue 0.523291 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76106 ms - Host latency: 1.85706 ms (enqueue 0.529688 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76274 ms - Host latency: 1.85867 ms (enqueue 0.521313 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.78179 ms - Host latency: 1.87773 ms (enqueue 0.514551 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.81714 ms - Host latency: 1.91328 ms (enqueue 0.53291 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76265 ms - Host latency: 1.85857 ms (enqueue 0.534351 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76265 ms - Host latency: 1.8585 ms (enqueue 0.508618 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.7624 ms - Host latency: 1.85815 ms (enqueue 0.532446 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.74365 ms - Host latency: 1.83955 ms (enqueue 0.517383 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76643 ms - Host latency: 1.8623 ms (enqueue 0.522876 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76238 ms - Host latency: 1.85828 ms (enqueue 0.528003 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76125 ms - Host latency: 1.8571 ms (enqueue 0.514893 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76096 ms - Host latency: 1.85684 ms (enqueue 0.523633 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.79736 ms - Host latency: 1.89431 ms (enqueue 0.538135 ms)\n",
      "[02/05/2024-14:51:17] [I] Average on 10 runs - GPU latency: 1.76448 ms - Host latency: 1.86028 ms (enqueue 0.535425 ms)\n",
      "[02/05/2024-14:51:17] [I] \n",
      "[02/05/2024-14:51:17] [I] === Performance summary ===\n",
      "[02/05/2024-14:51:17] [I] Throughput: 520.505 qps\n",
      "[02/05/2024-14:51:17] [I] Latency: min = 1.83228 ms, max = 2.55334 ms, mean = 1.86894 ms, median = 1.83862 ms, percentile(90%) = 2.03198 ms, percentile(95%) = 2.04224 ms, percentile(99%) = 2.36935 ms\n",
      "[02/05/2024-14:51:17] [I] Enqueue Time: min = 0.412598 ms, max = 1.46533 ms, mean = 0.535289 ms, median = 0.530029 ms, percentile(90%) = 0.572144 ms, percentile(95%) = 0.602905 ms, percentile(99%) = 0.715988 ms\n",
      "[02/05/2024-14:51:17] [I] H2D Latency: min = 0.0923462 ms, max = 0.101837 ms, mean = 0.0927147 ms, median = 0.0926514 ms, percentile(90%) = 0.0927734 ms, percentile(95%) = 0.0928955 ms, percentile(99%) = 0.0930176 ms\n",
      "[02/05/2024-14:51:17] [I] GPU Compute Time: min = 1.73682 ms, max = 2.45654 ms, mean = 1.77288 ms, median = 1.74268 ms, percentile(90%) = 1.93604 ms, percentile(95%) = 1.94556 ms, percentile(99%) = 2.272 ms\n",
      "[02/05/2024-14:51:17] [I] D2H Latency: min = 0.00292969 ms, max = 0.0161133 ms, mean = 0.00334306 ms, median = 0.00317383 ms, percentile(90%) = 0.00341797 ms, percentile(95%) = 0.00349426 ms, percentile(99%) = 0.00512695 ms\n",
      "[02/05/2024-14:51:17] [I] Total Host Walltime: 3.00285 s\n",
      "[02/05/2024-14:51:17] [I] Total GPU Compute Time: 2.77102 s\n",
      "[02/05/2024-14:51:17] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/05/2024-14:51:17] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02/05/2024-14:50:46] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[02/05/2024-14:50:46] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[02/05/2024-14:50:50] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/05/2024-14:51:17] [W] * GPU compute time is unstable, with coefficient of variance = 5.31548%.\n",
      "[02/05/2024-14:51:17] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    }
   ],
   "source": [
    "FP16 = False\n",
    "if FP16:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 预测结果： [490 490 490 490 490 490 490 490]\n",
      "TensorRT 推理时间： 0.0026176695823669433\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 通过builder创建一个网络\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "\n",
    "# 2. 解析ONNX文件并设置构建配置（需要使用普通未经过优化的onnx模型）\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model_dynamic.onnx\", \"rb\") as model:\n",
    "    parser.parse(model.read())\n",
    "config = builder.create_builder_config()  # 创建构建设置\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 设置内存大小\n",
    "profile = builder.create_optimization_profile()  # 创建优化配置\n",
    "min_shape = (1, 3, 112, 112)  # 设置最小尺寸\n",
    "opt_shape = (4, 3, 224, 224)  # 设置优先尺寸\n",
    "max_shape = (8, 3, 448, 448)  # 设置最大尺寸\n",
    "profile.set_shape(\"input\", min_shape, opt_shape, max_shape)\n",
    "config.add_optimization_profile(profile)  # 添加优化配置\n",
    "\n",
    "# 3. 转换并导出模型\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "# 4. 创建引擎和上下文管理器\n",
    "with open(\"model_dynamic.trt\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 5. 分配输入输出的内存\n",
    "stream = cuda.Stream()\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "output_data = np.empty([raw_data.shape[0], 1000], dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "context.set_input_shape(engine.get_tensor_name(0), input_data.shape)\n",
    "\n",
    "# 6. 推理和计时\n",
    "def predict(input_data):\n",
    "    # 将数据转换到驱动上去\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # 异步执行模型\n",
    "    context.execute_async_v2(bindings, stream.handle, None)  # 同步推理为execute_v2\n",
    "    # 将数据从驱动上转换回来\n",
    "    cuda.memcpy_dtoh_async(output_data, d_output, stream)\n",
    "    # 阻塞调用线程从而同步CUDA流\n",
    "    stream.synchronize()\n",
    "    return output_data\n",
    "\n",
    "dummy = True\n",
    "if dummy:\n",
    "    prediction = predict(input_data)\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    prediction = predict(input_data)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"TensorRT 预测结果：\", predicted_class)\n",
    "print(\"TensorRT 推理时间：\", (end_time - start_time) / num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
