{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1：基准测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch 推理结果： tensor([904, 904, 490, 490, 490, 904, 490, 490], device='cuda:0')\n",
      "Pytorch 推理时间： 0.18204164505004883\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import time\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 导出模型\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# 3. 输入数据\n",
    "input_tensor = torch.randn(8, 3, 224, 224).to(\"cuda\")\n",
    "\n",
    "# 4. 进行推理\n",
    "time_start = time.time()\n",
    "with torch.inference_mode():\n",
    "    output = model(input_tensor)\n",
    "time_end = time.time()\n",
    "print(\"Pytorch 推理结果：\", torch.argmax(output, dim=1))\n",
    "print(\"Pytorch 推理时间：\", time_end - time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2：使用 ONNX 优化模型\n",
    "\n",
    "（目前onnxruntime还不支持cuda12，无法启用gpu后端）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 推理结果： [904 904 490 490 490 904 490 490]\n",
      "ONNX 推理时间： 0.08050775527954102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 转换并导出静态和动态模型\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model.onnx\",\n",
    ")\n",
    "dynamic_axes = {\n",
    "    \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # 设置输入张量的名称是'input'，仅固定通道维度\n",
    "    \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # 设置输出张量的名称是'output'，仅固定通道维度\n",
    "}\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model_dynamic.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    ")\n",
    "\n",
    "# 2. 输入数据\n",
    "input_data = np.random.rand(8, 3, 224, 224).astype(np.float32)\n",
    "input_data = np.array(input_tensor.to(\"cpu\"), dtype=np.float32)\n",
    "\n",
    "# 3. 创建推理会话并\n",
    "sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "sess_options.graph_optimization_level = (\n",
    "    ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    ")  # 启用所有适用的图优化技术\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"  # 优化后的模型保存路径\n",
    "providers = [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]  # 设置执行提供者\n",
    "session = ort.InferenceSession(\"model_dynamic.onnx\", sess_options, providers=providers)\n",
    "\n",
    "# 4. 执行推理\n",
    "start_time = time.time()\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "output_data = session.run([output_name], {input_name: input_data})\n",
    "end_time = time.time()\n",
    "print(\"ONNX 推理结果：\", np.argmax(output_data[0], axis=1))\n",
    "print(\"ONNX 推理时间：\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3：使用 TensorRT 优化模型\n",
    "（目前存在未知bug，推理结果不正常）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP16 = False\n",
    "if FP16:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:1x3x224x224 --maxShapes=input:1x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:1x3x224x224 --maxShapes=input:1x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 预测结果： [490   0   0   0   0   0   0   0]\n",
      "TensorRT 推理时间： 0.0017352104187011719\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 通过builder创建一个网络\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "\n",
    "# 2. 解析ONNX文件并设置构建配置（需要使用普通未经过优化的onnx模型）\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model.onnx\", \"rb\") as model:\n",
    "    parser.parse(model.read())\n",
    "config = builder.create_builder_config()  # 创建构建设置\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 设置内存大小\n",
    "profile = builder.create_optimization_profile()  # 创建优化配置\n",
    "min_shape = (1, 3, 224, 224)  # 设置最小尺寸\n",
    "opt_shape = (4, 3, 256, 256)  # 设置优先尺寸\n",
    "max_shape = (8, 3, 384, 384)  # 设置最大尺寸\n",
    "profile.set_shape(\"input\", min_shape, opt_shape, max_shape)\n",
    "config.add_optimization_profile(profile)  # 添加优化配置\n",
    "\n",
    "# 3. 转换并导出模型\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "# 4. 创建引擎和上下文管理器\n",
    "with open(\"model.trt\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "# 5. 分配输入输出的内存\n",
    "BATCH_SIZE = 8\n",
    "input_data = np.random.randn(BATCH_SIZE, 3, 224, 224).astype(np.float32)\n",
    "input_data = np.array(input_tensor.to(\"cpu\"), dtype=np.float32)\n",
    "output_data = np.empty([BATCH_SIZE, 1000], dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(1 * input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "stream = cuda.Stream()\n",
    "\n",
    "\n",
    "# 6. 推理和计时\n",
    "def predict(input_data):\n",
    "    # 将数据转换到驱动上去\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # 异步执行模型\n",
    "    context.execute_async_v2(bindings, stream.handle, None)  # 同步推理为execute_v2\n",
    "    # 将数据从驱动上转换回来\n",
    "    cuda.memcpy_dtoh_async(output_data, d_output, stream)\n",
    "    # 阻塞调用线程从而同步CUDA流\n",
    "    stream.synchronize()\n",
    "    return output_data\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "prediction = predict(input_data)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"TensorRT 预测结果：\", predicted_class)\n",
    "print(\"TensorRT 推理时间：\", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
