{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1：基准测试 pytorch2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch 推理结果： tensor([904, 490, 490, 490, 490, 904, 490, 490], device='cuda:0')\n",
      "Pytorch 推理时间： 0.00404442024230957\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import time\n",
    "\n",
    "# 1. 加载预训练模型\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 导出模型\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# 3. 输入数据\n",
    "raw_data = torch.randn((8, 3, 224, 224), dtype=torch.float32, device=\"cpu\")\n",
    "input_tensor = raw_data.to(\"cuda\")\n",
    "\n",
    "# 4. 进行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    output = model(input_tensor)\n",
    "time_start = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "time_end = time.time()\n",
    "print(\"Pytorch 推理结果：\", predicted_class)\n",
    "print(\"Pytorch 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2：使用 ONNX1.7 优化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-02-04 17:15:30.938151905 [W:onnxruntime:Default, tensorrt_execution_provider.h:83 log] [2024-02-04 09:15:30 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\u001b[m\n",
      "\u001b[0;93m2024-02-04 17:15:30.960187934 [W:onnxruntime:Default, tensorrt_execution_provider.h:83 log] [2024-02-04 09:15:30 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 推理结果： [904 490 490 490 490 904 490 490]\n",
      "ONNX 推理时间： 0.00404442024230957\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 转换并导出静态和动态模型\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(8, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model.onnx\",\n",
    ")\n",
    "torch.onnx.export(\n",
    "    torch.load(\"model.pth\", map_location=torch.device(\"cpu\")),\n",
    "    torch.randn(1, 3, 224, 224).to(\"cpu\"),\n",
    "    \"model_dynamic.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输入张量的名称是'input'，仅固定通道维度\n",
    "        \"output\": {\n",
    "            0: \"batch_size\",\n",
    "            2: \"height\",\n",
    "            3: \"width\",\n",
    "        },  # 设置输出张量的名称是'output'，仅固定通道维度\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. 输入数据\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "\n",
    "# 3. 创建推理会话\n",
    "tensorrt_accelerate = False\n",
    "if tensorrt_accelerate:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    providers = [\n",
    "        \"TensorrtExecutionProvider\",\n",
    "        \"CUDAExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "else:\n",
    "    sess_options = ort.SessionOptions()  # 创建会话配置\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # 顺序执行\n",
    "    sess_options.graph_optimization_level = (\n",
    "        ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    )  # 启用所有适用的图优化技术\n",
    "    providers = [\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ]  # 设置执行提供者\n",
    "session = ort.InferenceSession(\"model_dynamic.onnx\", sess_options, providers=providers)\n",
    "\n",
    "# 4. 执行推理\n",
    "dummy = True\n",
    "if dummy:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for i in range(num):\n",
    "    output_data = session.run([output_name], {input_name: input_data})\n",
    "    predicted_class = np.argmax(output_data[0], axis=1)\n",
    "end_time = time.time()\n",
    "print(\"ONNX 推理结果：\", predicted_class)\n",
    "print(\"ONNX 推理时间：\", (time_end - time_start) / num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3：使用 TensorRT8.6 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: trtexec: command not found\n"
     ]
    }
   ],
   "source": [
    "FP16 = False\n",
    "if FP16:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=model_dynamic.onnx --minShapes=input:1x3x112x112 --optShapes=input:4x3x224x224 --maxShapes=input:8x3x448x448 --saveEngine=model_dynamic.trt --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 预测结果： [904 490 490 490 490 904 490 490]\n",
      "TensorRT 推理时间： 0.0027443642616271973\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. 通过builder创建一个网络\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "\n",
    "# 2. 解析ONNX文件并设置构建配置（需要使用普通未经过优化的onnx模型）\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model.onnx\", \"rb\") as model:\n",
    "    parser.parse(model.read())\n",
    "config = builder.create_builder_config()  # 创建构建设置\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 设置内存大小\n",
    "profile = builder.create_optimization_profile()  # 创建优化配置\n",
    "min_shape = (1, 3, 112, 112)  # 设置最小尺寸\n",
    "opt_shape = (4, 3, 224, 224)  # 设置优先尺寸\n",
    "max_shape = (8, 3, 448, 448)  # 设置最大尺寸\n",
    "profile.set_shape(\"input\", min_shape, opt_shape, max_shape)\n",
    "config.add_optimization_profile(profile)  # 添加优化配置\n",
    "\n",
    "# 3. 转换并导出模型\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "# 4. 创建引擎和上下文管理器\n",
    "with open(\"model.trt\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 5. 分配输入输出的内存\n",
    "stream = cuda.Stream()\n",
    "input_data = np.asarray(raw_data, dtype=np.float32)\n",
    "output_data = np.empty([raw_data.shape[0], 1000], dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "context.set_input_shape(engine.get_tensor_name(0), input_data.shape)\n",
    "\n",
    "# 6. 推理和计时\n",
    "def predict(input_data):\n",
    "    # 将数据转换到驱动上去\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # 异步执行模型\n",
    "    context.execute_async_v2(bindings, stream.handle, None)  # 同步推理为execute_v2\n",
    "    # 将数据从驱动上转换回来\n",
    "    cuda.memcpy_dtoh_async(output_data, d_output, stream)\n",
    "    # 阻塞调用线程从而同步CUDA流\n",
    "    stream.synchronize()\n",
    "    return output_data\n",
    "\n",
    "dummy = True\n",
    "if dummy:\n",
    "    prediction = predict(input_data)\n",
    "start_time = time.time()\n",
    "num = 1000\n",
    "for _ in range(num):\n",
    "    prediction = predict(input_data)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"TensorRT 预测结果：\", predicted_class)\n",
    "print(\"TensorRT 推理时间：\", (end_time - start_time) / num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
