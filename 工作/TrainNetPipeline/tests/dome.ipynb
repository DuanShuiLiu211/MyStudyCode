{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit_base_patch8_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.augreg_in21k',\n",
       " 'vit_base_patch8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.dino',\n",
       " 'vit_base_patch14_dinov2.lvd142m',\n",
       " 'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.augreg_in1k',\n",
       " 'vit_base_patch16_224.augreg_in21k',\n",
       " 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.dino',\n",
       " 'vit_base_patch16_224.mae',\n",
       " 'vit_base_patch16_224.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.sam_in1k',\n",
       " 'vit_base_patch16_224_miil.in21k',\n",
       " 'vit_base_patch16_224_miil.in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.augreg_in1k',\n",
       " 'vit_base_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.laion2b',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_224.openai',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_rpn_224.sw_in1k',\n",
       " 'vit_base_patch32_224.augreg_in1k',\n",
       " 'vit_base_patch32_224.augreg_in21k',\n",
       " 'vit_base_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_224.sam_in1k',\n",
       " 'vit_base_patch32_384.augreg_in1k',\n",
       " 'vit_base_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_224.openai',\n",
       " 'vit_base_patch32_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_r50_s16_224.orig_in21k',\n",
       " 'vit_base_r50_s16_384.orig_in21k_ft_in1k',\n",
       " 'vit_giant_patch14_clip_224.laion2b',\n",
       " 'vit_giant_patch14_dinov2.lvd142m',\n",
       " 'vit_gigantic_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_224.mae',\n",
       " 'vit_huge_patch14_224.orig_in21k',\n",
       " 'vit_huge_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_224.datacompxl',\n",
       " 'vit_large_patch14_clip_224.laion2b',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_224.openai',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.openai',\n",
       " 'vit_large_patch14_clip_336.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_dinov2.lvd142m',\n",
       " 'vit_large_patch16_224.augreg_in21k',\n",
       " 'vit_large_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch16_224.mae',\n",
       " 'vit_large_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch32_224.orig_in21k',\n",
       " 'vit_large_patch32_384.orig_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_240.sw_in12k',\n",
       " 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k',\n",
       " 'vit_relpos_base_patch16_224.sw_in1k',\n",
       " 'vit_relpos_base_patch16_clsgap_224.sw_in1k',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_cls_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_rpn_224.sw_in1k',\n",
       " 'vit_relpos_small_patch16_224.sw_in1k',\n",
       " 'vit_small_patch8_224.dino',\n",
       " 'vit_small_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch16_224.augreg_in1k',\n",
       " 'vit_small_patch16_224.augreg_in21k',\n",
       " 'vit_small_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch16_224.dino',\n",
       " 'vit_small_patch16_384.augreg_in1k',\n",
       " 'vit_small_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_224.augreg_in21k',\n",
       " 'vit_small_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_srelpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_srelpos_small_patch16_224.sw_in1k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(\"vit*\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'hf_hub_id': 'timm/eva02_base_patch14_224.mim_in22k', 'architecture': 'eva02_base_patch14_224', 'tag': 'mim_in22k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 0.9, 'crop_mode': 'center', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'num_classes': 0, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'license': 'mit'}\n",
      "Identity()\n",
      "Eva(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (rope): RotaryEmbeddingCat()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x EvaBlock(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): EvaAttention(\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SwiGLU(\n",
      "        (fc1_g): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (fc1_x): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (act): SiLU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"eva02_base_patch14_224.mim_in22k\", pretrained=True)\n",
    "print(model.default_cfg)\n",
    "print(model.get_classifier())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a79aad9e4df45b1ad7d018947281810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"model.safetensors\";:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'hf_hub_id': 'timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k', 'architecture': 'vit_base_patch16_clip_224', 'tag': 'laion2b_ft_in12k_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 0.95, 'crop_mode': 'center', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'num_classes': 1000, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head'}\n",
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_clip_224.laion2b_ft_in12k_in1k\", pretrained=True)\n",
    "print(model.default_cfg)\n",
    "print(model.get_classifier())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'hf_hub_id': 'timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k', 'architecture': 'vit_base_patch16_clip_224', 'tag': 'laion2b_ft_in12k_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 0.95, 'crop_mode': 'center', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'num_classes': 1000, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head'}\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=3, bias=False)\n",
      "    (4): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=256, bias=False)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=3, bias=False)\n",
      "  (4): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_clip_224.laion2b_ft_in12k_in1k\", pretrained=False) \n",
    "model.head = nn.Sequential(# nn.BatchNorm1d(2048),\n",
    "                        nn.Linear(in_features=768, out_features=256, bias=False),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # nn.BatchNorm1d(256),\n",
    "                        nn.Dropout(0.4),\n",
    "                        nn.Linear(in_features=256, out_features=3, bias=False),\n",
    "                        nn.LogSoftmax(dim=1))\n",
    "print(model.default_cfg)    \n",
    "print(model)   \n",
    "print(model.get_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth', 'hf_hub_id': 'timm/resnet50.fb_ssl_yfcc100m_ft_in1k', 'architecture': 'resnet50', 'tag': 'fb_ssl_yfcc100m_ft_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': False, 'interpolation': 'bilinear', 'crop_pct': 0.875, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 1000, 'pool_size': (7, 7), 'first_conv': 'conv1', 'classifier': 'fc', 'license': 'cc-by-nc-4.0', 'origin_url': 'https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'}\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"resnet50.fb_ssl_yfcc100m_ft_in1k\", pretrained=True)\n",
    "print(model.default_cfg)\n",
    "print(model.get_classifier())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth', 'hf_hub_id': 'timm/resnet50.fb_ssl_yfcc100m_ft_in1k', 'architecture': 'resnet50', 'tag': 'fb_ssl_yfcc100m_ft_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': False, 'interpolation': 'bilinear', 'crop_pct': 0.875, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 1000, 'pool_size': (7, 7), 'first_conv': 'conv1', 'classifier': 'fc', 'license': 'cc-by-nc-4.0', 'origin_url': 'https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'}\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=3, bias=False)\n",
      "    (4): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=3, bias=False)\n",
      "  (4): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"resnet50.fb_ssl_yfcc100m_ft_in1k\", pretrained=False, num_classes=3)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=2048, out_features=256, bias=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=256, out_features=3, bias=False),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "print(model.default_cfg)\n",
    "print(model)\n",
    "print(model.get_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]]) None\n"
     ]
    }
   ],
   "source": [
    "l1 = nn.Linear(2, 3, bias=False)\n",
    "l1.weight.data = 2 * torch.ones((3, 2))\n",
    "print(l1.weight.data, l1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.6686,  1.7234],\n",
      "           [ 1.0820, -0.7666]],\n",
      "\n",
      "          [[-0.6063, -0.6130],\n",
      "           [ 0.7127, -0.8715]]],\n",
      "\n",
      "\n",
      "         [[[-0.2614,  1.1844],\n",
      "           [-0.4491, -0.0843]],\n",
      "\n",
      "          [[-0.5783,  0.2204],\n",
      "           [ 0.0388, -0.1187]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.3207,  1.4489],\n",
      "           [ 1.9010, -0.4401]],\n",
      "\n",
      "          [[ 0.3106, -0.3101],\n",
      "           [-1.2597, -0.2923]]],\n",
      "\n",
      "\n",
      "         [[[-1.2101, -1.4021],\n",
      "           [ 0.2772,  0.3286]],\n",
      "\n",
      "          [[ 0.3671,  0.9998],\n",
      "           [-0.9491, -1.3918]]]]])\n"
     ]
    }
   ],
   "source": [
    "d1 = torch.randn((2, 2, 2, 2, 2))\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 4.7840e+00,  4.7840e+00,  4.7840e+00],\n",
      "           [ 6.3068e-01,  6.3068e-01,  6.3068e-01]],\n",
      "\n",
      "          [[-2.4386e+00, -2.4386e+00, -2.4386e+00],\n",
      "           [-3.1757e-01, -3.1757e-01, -3.1757e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8461e+00,  1.8461e+00,  1.8461e+00],\n",
      "           [-1.0668e+00, -1.0668e+00, -1.0668e+00]],\n",
      "\n",
      "          [[-7.1580e-01, -7.1580e-01, -7.1580e-01],\n",
      "           [-1.5983e-01, -1.5983e-01, -1.5983e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.2564e+00,  2.2564e+00,  2.2564e+00],\n",
      "           [ 2.9217e+00,  2.9217e+00,  2.9217e+00]],\n",
      "\n",
      "          [[ 9.9802e-04,  9.9802e-04,  9.9802e-04],\n",
      "           [-3.1040e+00, -3.1040e+00, -3.1040e+00]]],\n",
      "\n",
      "\n",
      "         [[[-5.2244e+00, -5.2244e+00, -5.2244e+00],\n",
      "           [ 1.2115e+00,  1.2115e+00,  1.2115e+00]],\n",
      "\n",
      "          [[ 2.7337e+00,  2.7337e+00,  2.7337e+00],\n",
      "           [-4.6819e+00, -4.6819e+00, -4.6819e+00]]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d2 = l1(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a data = (tensor(2.), True), grad = None\n",
      "b data = (tensor(3.), False), grad = None\n",
      "c data = (tensor(6.), True), grad = None\n",
      "d data = (tensor(4.), True), grad = None\n",
      "e data = (tensor(24.), True), grad = None\n",
      "f data = (tensor(24.), True), grad = None\n",
      "g data = (tensor(5.), False), grad = None\n",
      "h data = (tensor(120.), True), grad = None\n",
      "i data = (tensor(144.), True), grad = None\n",
      "a data = (tensor(2.), True), grad = 72.0\n",
      "b data = (tensor(3.), False), grad = None\n",
      "c data = (tensor(6.), True), grad = None\n",
      "d data = (tensor(4.), True), grad = 36.0\n",
      "e data = (tensor(24.), True), grad = 1.0\n",
      "f data = (tensor(24.), True), grad = 5.0\n",
      "g data = (tensor(5.), False), grad = None\n",
      "h data = (tensor(120.), True), grad = None\n",
      "i data = (tensor(144.), True), grad = None\n"
     ]
    }
   ],
   "source": [
    "a = torch.nn.Parameter(torch.tensor(2.))\n",
    "b = torch.tensor(3.)\n",
    "c = a * b\n",
    "d = torch.nn.Parameter(torch.tensor(4.))\n",
    "e = c * d\n",
    "# e.detach_()\n",
    "f = c * d\n",
    "g = torch.nn.Parameter(torch.tensor(5.))\n",
    "g.detach_()\n",
    "h = f * g\n",
    "i = e + h\n",
    "print(f\"a data = {a.data,a.requires_grad}, grad = {a.grad}\")\n",
    "print(f\"b data = {b.data,b.requires_grad}, grad = {b.grad}\")\n",
    "print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
    "print(f\"d data = {d.data,d.requires_grad}, grad = {d.grad}\")\n",
    "print(f\"e data = {e.data,e.requires_grad}, grad = {e.grad}\")\n",
    "print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
    "print(f\"g data = {g.data,g.requires_grad}, grad = {g.grad}\")\n",
    "print(f\"h data = {h.data,h.requires_grad}, grad = {h.grad}\")\n",
    "print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")\n",
    "\n",
    "# h.requires_grad_(False)\n",
    "# h.detach_()\n",
    "g.detach_()\n",
    "f.retain_grad()\n",
    "e.retain_grad()\n",
    "i.backward()\n",
    "\n",
    "print(f\"a data = {a.data,a.requires_grad}, grad = {a.grad}\")\n",
    "print(f\"b data = {b.data,b.requires_grad}, grad = {b.grad}\")\n",
    "print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
    "print(f\"d data = {d.data,d.requires_grad}, grad = {d.grad}\")\n",
    "print(f\"e data = {e.data,e.requires_grad}, grad = {e.grad}\")\n",
    "print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
    "print(f\"g data = {g.data,g.requires_grad}, grad = {g.grad}\")\n",
    "print(f\"h data = {h.data,h.requires_grad}, grad = {h.grad}\")\n",
    "print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True) tensor([1.])\n",
      "tensor([2.], grad_fn=<MulBackward0>)\n",
      "tensor([100.], requires_grad=True) tensor([2.])\n",
      "tensor([2.], grad_fn=<MulBackward0>)\n",
      "tensor([100.], requires_grad=True) tensor([100.])\n",
      "tensor([200.], grad_fn=<MulBackward0>)\n",
      "tensor([10000.], requires_grad=True) tensor([4.])\n",
      "tensor([200.], grad_fn=<MulBackward0>)\n",
      "tensor([10000.], requires_grad=True) tensor([10000.])\n",
      "tensor([20000.], grad_fn=<MulBackward0>)\n",
      "tensor([1000000.], requires_grad=True) tensor([6.])\n",
      "tensor([20000.], grad_fn=<MulBackward0>)\n",
      "tensor([1000000.], requires_grad=True) tensor([1000000.])\n",
      "tensor([2000000.], grad_fn=<MulBackward0>)\n",
      "tensor([100000000.], requires_grad=True) tensor([8.])\n",
      "tensor([2000000.], grad_fn=<MulBackward0>)\n",
      "tensor([100000000.], requires_grad=True) tensor([100000000.])\n",
      "tensor([2.0000e+08], grad_fn=<MulBackward0>)\n",
      "tensor([1.0000e+10], requires_grad=True) tensor([10.])\n",
      "tensor([2.0000e+08], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "for i in range(5):\n",
    "    print(x, x.data)  # 存储x的数据\n",
    "    y = 2 * x\n",
    "    print(y)\n",
    "    reg = x.data * 100\n",
    "    x.data = reg  # 只改变数据，不会改变计算图，但是自动微分机制被破坏了，将无法正确优化\n",
    "    y.backward()\n",
    "    print(x, x.grad)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c data: tensor(7.5000, grad_fn=<MeanBackward0>)\n",
      "a grad: tensor([1., 1.]) True\n",
      "b grad: None True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tensor/lib/python3.9/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1656352453927/work/build/aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "a = torch.nn.Parameter(torch.tensor([1., 2.]))\n",
    "b=2*a\n",
    "b.detach_()\n",
    "d = 3*a\n",
    "c =torch.mean(d+b)\n",
    "print(\"c data:\",c)\n",
    "c.backward()\n",
    "print(\"a grad:\", a.grad, a.requires_grad)\n",
    "print(\"b grad:\", b.grad, b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (befortraining) 4 4.0\n",
      "Epoch: 0\n",
      "x is 1.0, y is 2.0，l is 1.0\n",
      "w is 1.0, w grad is -2.0\n",
      "x is 2.0, y is 4.0，l is 3.841600179672241\n",
      "w is 1.0199999809265137, w grad is -7.840000152587891\n",
      "x is 3.0, y is 6.0，l is 7.315943717956543\n",
      "w is 1.0983999967575073, w grad is -16.228801727294922\n",
      "Epoch: 1\n",
      "x is 1.0, y is 2.0，l is 0.5465821623802185\n",
      "w is 1.260688066482544, w grad is -1.478623867034912\n",
      "x is 2.0, y is 4.0，l is 2.099749803543091\n",
      "w is 1.2754743099212646, w grad is -5.796205520629883\n",
      "x is 3.0, y is 6.0，l is 3.9987640380859375\n",
      "w is 1.333436369895935, w grad is -11.998146057128906\n",
      "Epoch: 2\n",
      "x is 1.0, y is 2.0，l is 0.2987521290779114\n",
      "w is 1.4534177780151367, w grad is -1.0931644439697266\n",
      "x is 2.0, y is 4.0，l is 1.1476863622665405\n",
      "w is 1.464349389076233, w grad is -4.285204887390137\n",
      "x is 3.0, y is 6.0，l is 2.1856532096862793\n",
      "w is 1.5072014331817627, w grad is -8.870372772216797\n",
      "Epoch: 3\n",
      "x is 1.0, y is 2.0，l is 0.16329261660575867\n",
      "w is 1.5959051847457886, w grad is -0.8081896305084229\n",
      "x is 2.0, y is 4.0，l is 0.6273048520088196\n",
      "w is 1.6039870977401733, w grad is -3.1681032180786133\n",
      "x is 3.0, y is 6.0，l is 1.1946394443511963\n",
      "w is 1.635668158531189, w grad is -6.557973861694336\n",
      "Epoch: 4\n",
      "x is 1.0, y is 2.0，l is 0.08925279974937439\n",
      "w is 1.7012479305267334, w grad is -0.5975041389465332\n",
      "x is 2.0, y is 4.0，l is 0.34287363290786743\n",
      "w is 1.7072229385375977, w grad is -2.3422164916992188\n",
      "x is 3.0, y is 6.0，l is 0.6529689431190491\n",
      "w is 1.7306450605392456, w grad is -4.848389625549316\n",
      "Epoch: 5\n",
      "x is 1.0, y is 2.0，l is 0.048784039914608\n",
      "w is 1.779128909111023, w grad is -0.4417421817779541\n",
      "x is 2.0, y is 4.0，l is 0.18740876019001007\n",
      "w is 1.7835463285446167, w grad is -1.7316293716430664\n",
      "x is 3.0, y is 6.0，l is 0.35690122842788696\n",
      "w is 1.8008626699447632, w grad is -3.58447265625\n",
      "Epoch: 6\n",
      "x is 1.0, y is 2.0，l is 0.02666448801755905\n",
      "w is 1.836707353591919, w grad is -0.3265852928161621\n",
      "x is 2.0, y is 4.0，l is 0.10243429243564606\n",
      "w is 1.8399732112884521, w grad is -1.2802143096923828\n",
      "x is 3.0, y is 6.0，l is 0.195076122879982\n",
      "w is 1.8527753353118896, w grad is -2.650045394897461\n",
      "Epoch: 7\n",
      "x is 1.0, y is 2.0，l is 0.014574333094060421\n",
      "w is 1.8792757987976074, w grad is -0.24144840240478516\n",
      "x is 2.0, y is 4.0，l is 0.055988773703575134\n",
      "w is 1.881690263748169, w grad is -0.9464778900146484\n",
      "x is 3.0, y is 6.0，l is 0.10662525147199631\n",
      "w is 1.8911550045013428, w grad is -1.9592113494873047\n",
      "Epoch: 8\n",
      "x is 1.0, y is 2.0，l is 0.007966067641973495\n",
      "w is 1.9107471704483032, w grad is -0.17850565910339355\n",
      "x is 2.0, y is 4.0，l is 0.030602457001805305\n",
      "w is 1.9125322103500366, w grad is -0.699742317199707\n",
      "x is 3.0, y is 6.0，l is 0.0582793727517128\n",
      "w is 1.919529676437378, w grad is -1.4484672546386719\n",
      "Epoch: 9\n",
      "x is 1.0, y is 2.0，l is 0.004354109987616539\n",
      "w is 1.9340143203735352, w grad is -0.1319713592529297\n",
      "x is 2.0, y is 4.0，l is 0.016726721078157425\n",
      "w is 1.9353340864181519, w grad is -0.5173273086547852\n",
      "x is 3.0, y is 6.0，l is 0.03185431286692619\n",
      "w is 1.940507411956787, w grad is -1.070866584777832\n",
      "Epoch: 10\n",
      "x is 1.0, y is 2.0，l is 0.002379868645220995\n",
      "w is 1.9512161016464233, w grad is -0.09756779670715332\n",
      "x is 2.0, y is 4.0，l is 0.00914248451590538\n",
      "w is 1.9521918296813965, w grad is -0.3824653625488281\n",
      "x is 3.0, y is 6.0，l is 0.017410902306437492\n",
      "w is 1.9560165405273438, w grad is -0.7917022705078125\n",
      "Epoch: 11\n",
      "x is 1.0, y is 2.0，l is 0.001300786156207323\n",
      "w is 1.9639335870742798, w grad is -0.07213282585144043\n",
      "x is 2.0, y is 4.0，l is 0.004997097887098789\n",
      "w is 1.9646549224853516, w grad is -0.2827606201171875\n",
      "x is 3.0, y is 6.0，l is 0.009516451507806778\n",
      "w is 1.967482566833496, w grad is -0.5853137969970703\n",
      "Epoch: 12\n",
      "x is 1.0, y is 2.0，l is 0.000710982596501708\n",
      "w is 1.9733357429504395, w grad is -0.053328514099121094\n",
      "x is 2.0, y is 4.0，l is 0.0027312987949699163\n",
      "w is 1.9738690853118896, w grad is -0.2090473175048828\n",
      "x is 3.0, y is 6.0，l is 0.005201528314501047\n",
      "w is 1.9759595394134521, w grad is -0.43272972106933594\n",
      "Epoch: 13\n",
      "x is 1.0, y is 2.0，l is 0.0003886088088620454\n",
      "w is 1.9802868366241455, w grad is -0.039426326751708984\n",
      "x is 2.0, y is 4.0，l is 0.0014928855234757066\n",
      "w is 1.98068106174469, w grad is -0.15455150604248047\n",
      "x is 3.0, y is 6.0，l is 0.0028430151287466288\n",
      "w is 1.9822266101837158, w grad is -0.3199195861816406\n",
      "Epoch: 14\n",
      "x is 1.0, y is 2.0，l is 0.0002124064340023324\n",
      "w is 1.9854258298873901, w grad is -0.029148340225219727\n",
      "x is 2.0, y is 4.0，l is 0.0008159824647009373\n",
      "w is 1.9857172966003418, w grad is -0.11426162719726562\n",
      "x is 3.0, y is 6.0，l is 0.0015539465239271522\n",
      "w is 1.986859917640686, w grad is -0.23652076721191406\n",
      "Epoch: 15\n",
      "x is 1.0, y is 2.0，l is 0.0001160974134108983\n",
      "w is 1.989225149154663, w grad is -0.021549701690673828\n",
      "x is 2.0, y is 4.0，l is 0.00044599699322134256\n",
      "w is 1.989440679550171, w grad is -0.08447456359863281\n",
      "x is 3.0, y is 6.0，l is 0.0008493617060594261\n",
      "w is 1.9902853965759277, w grad is -0.17486286163330078\n",
      "Epoch: 16\n",
      "x is 1.0, y is 2.0，l is 6.345591827994213e-05\n",
      "w is 1.9920340776443481, w grad is -0.01593184471130371\n",
      "x is 2.0, y is 4.0，l is 0.0002437756775179878\n",
      "w is 1.992193341255188, w grad is -0.062453269958496094\n",
      "x is 3.0, y is 6.0，l is 0.00046424579340964556\n",
      "w is 1.9928178787231445, w grad is -0.12927818298339844\n",
      "Epoch: 17\n",
      "x is 1.0, y is 2.0，l is 3.468381328275427e-05\n",
      "w is 1.9941107034683228, w grad is -0.011778593063354492\n",
      "x is 2.0, y is 4.0，l is 0.00013324167230166495\n",
      "w is 1.994228482246399, w grad is -0.046172142028808594\n",
      "x is 3.0, y is 6.0，l is 0.0002537401160225272\n",
      "w is 1.994690179824829, w grad is -0.09557533264160156\n",
      "Epoch: 18\n",
      "x is 1.0, y is 2.0，l is 1.895835521281697e-05\n",
      "w is 1.9956458806991577, w grad is -0.00870823860168457\n",
      "x is 2.0, y is 4.0，l is 7.282838487299159e-05\n",
      "w is 1.9957330226898193, w grad is -0.03413581848144531\n",
      "x is 3.0, y is 6.0，l is 0.00013869594840798527\n",
      "w is 1.9960744380950928, w grad is -0.07066154479980469\n",
      "Epoch: 19\n",
      "x is 1.0, y is 2.0，l is 1.0361248314438853e-05\n",
      "w is 1.9967811107635498, w grad is -0.006437778472900391\n",
      "x is 2.0, y is 4.0，l is 3.980389010393992e-05\n",
      "w is 1.9968454837799072, w grad is -0.025236129760742188\n",
      "x is 3.0, y is 6.0，l is 7.580435340059921e-05\n",
      "w is 1.9970978498458862, w grad is -0.052239418029785156\n",
      "Epoch: 20\n",
      "x is 1.0, y is 2.0，l is 5.663329375238391e-06\n",
      "w is 1.9976202249526978, w grad is -0.004759550094604492\n",
      "x is 2.0, y is 4.0，l is 2.1756823116447777e-05\n",
      "w is 1.9976677894592285, w grad is -0.018657684326171875\n",
      "x is 3.0, y is 6.0，l is 4.143271507928148e-05\n",
      "w is 1.9978543519973755, w grad is -0.038620948791503906\n",
      "Epoch: 21\n",
      "x is 1.0, y is 2.0，l is 3.0955231977713993e-06\n",
      "w is 1.99824059009552, w grad is -0.003518819808959961\n",
      "x is 2.0, y is 4.0，l is 1.1892057955265045e-05\n",
      "w is 1.9982757568359375, w grad is -0.0137939453125\n",
      "x is 3.0, y is 6.0，l is 2.264650902361609e-05\n",
      "w is 1.9984136819839478, w grad is -0.028553009033203125\n",
      "Epoch: 22\n",
      "x is 1.0, y is 2.0，l is 1.692111254669726e-06\n",
      "w is 1.9986991882324219, w grad is -0.00260162353515625\n",
      "x is 2.0, y is 4.0，l is 6.500706149381585e-06\n",
      "w is 1.998725175857544, w grad is -0.010198593139648438\n",
      "x is 3.0, y is 6.0，l is 1.2377059647405986e-05\n",
      "w is 1.9988272190093994, w grad is -0.021108627319335938\n",
      "Epoch: 23\n",
      "x is 1.0, y is 2.0，l is 9.247925163435866e-07\n",
      "w is 1.9990383386611938, w grad is -0.0019233226776123047\n",
      "x is 2.0, y is 4.0，l is 3.552988573574112e-06\n",
      "w is 1.9990575313568115, w grad is -0.0075397491455078125\n",
      "x is 3.0, y is 6.0，l is 6.768445018678904e-06\n",
      "w is 1.9991328716278076, w grad is -0.0156097412109375\n",
      "Epoch: 24\n",
      "x is 1.0, y is 2.0，l is 5.056396048530587e-07\n",
      "w is 1.9992889165878296, w grad is -0.0014221668243408203\n",
      "x is 2.0, y is 4.0，l is 1.9426645394560182e-06\n",
      "w is 1.9993031024932861, w grad is -0.0055751800537109375\n",
      "x is 3.0, y is 6.0，l is 3.7000872907810844e-06\n",
      "w is 1.999358892440796, w grad is -0.011541366577148438\n",
      "Epoch: 25\n",
      "x is 1.0, y is 2.0，l is 2.763741235867201e-07\n",
      "w is 1.999474287033081, w grad is -0.0010514259338378906\n",
      "x is 2.0, y is 4.0，l is 1.0618171017995337e-06\n",
      "w is 1.9994847774505615, w grad is -0.0041217803955078125\n",
      "x is 3.0, y is 6.0，l is 2.021880391112063e-06\n",
      "w is 1.999526023864746, w grad is -0.008531570434570312\n",
      "Epoch: 26\n",
      "x is 1.0, y is 2.0，l is 1.5102727957128081e-07\n",
      "w is 1.9996113777160645, w grad is -0.0007772445678710938\n",
      "x is 2.0, y is 4.0，l is 5.802590408165997e-07\n",
      "w is 1.9996191263198853, w grad is -0.0030469894409179688\n",
      "x is 3.0, y is 6.0，l is 1.1044940038118511e-06\n",
      "w is 1.9996496438980103, w grad is -0.006305694580078125\n",
      "Epoch: 27\n",
      "x is 1.0, y is 2.0，l is 8.253806527136476e-08\n",
      "w is 1.9997127056121826, w grad is -0.0005745887756347656\n",
      "x is 2.0, y is 4.0，l is 3.171319349348778e-07\n",
      "w is 1.999718427658081, w grad is -0.0022525787353515625\n",
      "x is 3.0, y is 6.0，l is 6.041091182851233e-07\n",
      "w is 1.9997409582138062, w grad is -0.0046634674072265625\n",
      "Epoch: 28\n",
      "x is 1.0, y is 2.0，l is 4.512691020863713e-08\n",
      "w is 1.9997875690460205, w grad is -0.0004248619079589844\n",
      "x is 2.0, y is 4.0，l is 1.73288071891875e-07\n",
      "w is 1.9997918605804443, w grad is -0.0016651153564453125\n",
      "x is 3.0, y is 6.0，l is 3.296045179013163e-07\n",
      "w is 1.9998085498809814, w grad is -0.003444671630859375\n",
      "Epoch: 29\n",
      "x is 1.0, y is 2.0，l is 2.4648571184116008e-08\n",
      "w is 1.9998430013656616, w grad is -0.0003139972686767578\n",
      "x is 2.0, y is 4.0，l is 9.473984619035036e-08\n",
      "w is 1.99984610080719, w grad is -0.0012311935424804688\n",
      "x is 3.0, y is 6.0，l is 1.805076408345485e-07\n",
      "w is 1.9998583793640137, w grad is -0.0025491714477539062\n",
      "Epoch: 30\n",
      "x is 1.0, y is 2.0，l is 1.3481496807798976e-08\n",
      "w is 1.9998838901519775, w grad is -0.00023221969604492188\n",
      "x is 2.0, y is 4.0，l is 5.184261908652843e-08\n",
      "w is 1.999886155128479, w grad is -0.0009107589721679688\n",
      "x is 3.0, y is 6.0，l is 9.874406714516226e-08\n",
      "w is 1.9998952150344849, w grad is -0.0018854141235351562\n",
      "Epoch: 31\n",
      "x is 1.0, y is 2.0，l is 7.387384926005325e-09\n",
      "w is 1.9999140501022339, w grad is -0.00017189979553222656\n",
      "x is 2.0, y is 4.0，l is 2.8413126074156025e-08\n",
      "w is 1.9999157190322876, w grad is -0.0006742477416992188\n",
      "x is 3.0, y is 6.0，l is 5.4147676564753056e-08\n",
      "w is 1.999922513961792, w grad is -0.00139617919921875\n",
      "Epoch: 32\n",
      "x is 1.0, y is 2.0，l is 4.037147505187022e-09\n",
      "w is 1.9999364614486694, w grad is -0.0001270771026611328\n",
      "x is 2.0, y is 4.0，l is 1.548892214486841e-08\n",
      "w is 1.9999377727508545, w grad is -0.0004978179931640625\n",
      "x is 3.0, y is 6.0，l is 2.9467628337442875e-08\n",
      "w is 1.9999427795410156, w grad is -0.00102996826171875\n",
      "Epoch: 33\n",
      "x is 1.0, y is 2.0，l is 2.2060362425690982e-09\n",
      "w is 1.999953031539917, w grad is -9.393692016601562e-05\n",
      "x is 2.0, y is 4.0，l is 8.469442036584951e-09\n",
      "w is 1.9999539852142334, w grad is -0.0003681182861328125\n",
      "x is 3.0, y is 6.0，l is 1.6088051779661328e-08\n",
      "w is 1.9999576807022095, w grad is -0.0007610321044921875\n",
      "Epoch: 34\n",
      "x is 1.0, y is 2.0，l is 1.2033893881380209e-09\n",
      "w is 1.9999653100967407, w grad is -6.937980651855469e-05\n",
      "x is 2.0, y is 4.0，l is 4.617106696969131e-09\n",
      "w is 1.999966025352478, w grad is -0.00027179718017578125\n",
      "x is 3.0, y is 6.0，l is 8.734787115827203e-09\n",
      "w is 1.9999687671661377, w grad is -0.000560760498046875\n",
      "Epoch: 35\n",
      "x is 1.0, y is 2.0，l is 6.568967592102126e-10\n",
      "w is 1.9999743700027466, w grad is -5.125999450683594e-05\n",
      "x is 2.0, y is 4.0，l is 2.5307258511020336e-09\n",
      "w is 1.9999748468399048, w grad is -0.00020122528076171875\n",
      "x is 3.0, y is 6.0，l is 4.8466972657479346e-09\n",
      "w is 1.9999768733978271, w grad is -0.0004177093505859375\n",
      "Epoch: 36\n",
      "x is 1.0, y is 2.0，l is 3.5926461805502186e-10\n",
      "w is 1.9999810457229614, w grad is -3.790855407714844e-05\n",
      "x is 2.0, y is 4.0，l is 1.3833414413966238e-09\n",
      "w is 1.99998140335083, w grad is -0.000148773193359375\n",
      "x is 3.0, y is 6.0，l is 2.6520865503698587e-09\n",
      "w is 1.9999828338623047, w grad is -0.000308990478515625\n",
      "Epoch: 37\n",
      "x is 1.0, y is 2.0，l is 1.978719410544727e-10\n",
      "w is 1.999985933303833, w grad is -2.8133392333984375e-05\n",
      "x is 2.0, y is 4.0，l is 7.648850441910326e-10\n",
      "w is 1.999986171722412, w grad is -0.000110626220703125\n",
      "x is 3.0, y is 6.0，l is 1.4551915228366852e-09\n",
      "w is 1.999987244606018, w grad is -0.0002288818359375\n",
      "Epoch: 38\n",
      "x is 1.0, y is 2.0，l is 1.1004885891452432e-10\n",
      "w is 1.9999895095825195, w grad is -2.09808349609375e-05\n",
      "x is 2.0, y is 4.0，l is 4.204139258945361e-10\n",
      "w is 1.9999897480010986, w grad is -8.20159912109375e-05\n",
      "x is 3.0, y is 6.0，l is 7.914877642178908e-10\n",
      "w is 1.9999905824661255, w grad is -0.00016880035400390625\n",
      "Epoch: 39\n",
      "x is 1.0, y is 2.0，l is 6.004086117172847e-11\n",
      "w is 1.9999922513961792, w grad is -1.5497207641601562e-05\n",
      "x is 2.0, y is 4.0，l is 2.3283064365386963e-10\n",
      "w is 1.9999923706054688, w grad is -6.103515625e-05\n",
      "x is 3.0, y is 6.0，l is 4.4019543565809727e-10\n",
      "w is 1.9999929666519165, w grad is -0.000125885009765625\n",
      "Epoch: 40\n",
      "x is 1.0, y is 2.0，l is 3.2741809263825417e-11\n",
      "w is 1.9999942779541016, w grad is -1.1444091796875e-05\n",
      "x is 2.0, y is 4.0，l is 1.255671122635249e-10\n",
      "w is 1.9999943971633911, w grad is -4.482269287109375e-05\n",
      "x is 3.0, y is 6.0，l is 2.3283064365386963e-10\n",
      "w is 1.9999948740005493, w grad is -9.1552734375e-05\n",
      "Epoch: 41\n",
      "x is 1.0, y is 2.0，l is 1.7408297026122455e-11\n",
      "w is 1.9999958276748657, w grad is -8.344650268554688e-06\n",
      "x is 2.0, y is 4.0，l is 6.571099220309407e-11\n",
      "w is 1.9999959468841553, w grad is -3.24249267578125e-05\n",
      "x is 3.0, y is 6.0，l is 1.2028067430946976e-10\n",
      "w is 1.999996304512024, w grad is -6.580352783203125e-05\n",
      "Epoch: 42\n",
      "x is 1.0, y is 2.0，l is 8.881784197001252e-12\n",
      "w is 1.9999970197677612, w grad is -5.9604644775390625e-06\n",
      "x is 2.0, y is 4.0，l is 3.2741809263825417e-11\n",
      "w is 1.9999971389770508, w grad is -2.288818359375e-05\n",
      "x is 3.0, y is 6.0，l is 5.820766091346741e-11\n",
      "w is 1.9999973773956299, w grad is -4.57763671875e-05\n",
      "Epoch: 43\n",
      "x is 1.0, y is 2.0，l is 4.604316927725449e-12\n",
      "w is 1.999997854232788, w grad is -4.291534423828125e-06\n",
      "x is 2.0, y is 4.0，l is 1.8417267710901797e-11\n",
      "w is 1.999997854232788, w grad is -1.71661376953125e-05\n",
      "x is 3.0, y is 6.0，l is 3.842615114990622e-11\n",
      "w is 1.9999979734420776, w grad is -3.719329833984375e-05\n",
      "Epoch: 44\n",
      "x is 1.0, y is 2.0，l is 2.7853275241795927e-12\n",
      "w is 1.9999983310699463, w grad is -3.337860107421875e-06\n",
      "x is 2.0, y is 4.0，l is 1.1141310096718371e-11\n",
      "w is 1.9999983310699463, w grad is -1.33514404296875e-05\n",
      "x is 3.0, y is 6.0，l is 2.2737367544323206e-11\n",
      "w is 1.9999984502792358, w grad is -2.86102294921875e-05\n",
      "Epoch: 45\n",
      "x is 1.0, y is 2.0，l is 1.7195134205394424e-12\n",
      "w is 1.999998688697815, w grad is -2.6226043701171875e-06\n",
      "x is 2.0, y is 4.0，l is 6.87805368215777e-12\n",
      "w is 1.999998688697815, w grad is -1.049041748046875e-05\n",
      "x is 3.0, y is 6.0，l is 1.4551915228366852e-11\n",
      "w is 1.9999988079071045, w grad is -2.288818359375e-05\n",
      "Epoch: 46\n",
      "x is 1.0, y is 2.0，l is 9.094947017729282e-13\n",
      "w is 1.9999990463256836, w grad is -1.9073486328125e-06\n",
      "x is 2.0, y is 4.0，l is 3.637978807091713e-12\n",
      "w is 1.9999990463256836, w grad is -7.62939453125e-06\n",
      "x is 3.0, y is 6.0，l is 5.6843418860808015e-12\n",
      "w is 1.9999991655349731, w grad is -1.430511474609375e-05\n",
      "Epoch: 47\n",
      "x is 1.0, y is 2.0，l is 5.115907697472721e-13\n",
      "w is 1.9999992847442627, w grad is -1.430511474609375e-06\n",
      "x is 2.0, y is 4.0，l is 2.0463630789890885e-12\n",
      "w is 1.9999992847442627, w grad is -5.7220458984375e-06\n",
      "x is 3.0, y is 6.0，l is 3.637978807091713e-12\n",
      "w is 1.9999992847442627, w grad is -1.1444091796875e-05\n",
      "Epoch: 48\n",
      "x is 1.0, y is 2.0，l is 3.552713678800501e-13\n",
      "w is 1.9999994039535522, w grad is -1.1920928955078125e-06\n",
      "x is 2.0, y is 4.0，l is 1.4210854715202004e-12\n",
      "w is 1.9999994039535522, w grad is -4.76837158203125e-06\n",
      "x is 3.0, y is 6.0，l is 3.637978807091713e-12\n",
      "w is 1.9999994039535522, w grad is -1.1444091796875e-05\n",
      "Epoch: 49\n",
      "x is 1.0, y is 2.0，l is 2.2737367544323206e-13\n",
      "w is 1.9999995231628418, w grad is -9.5367431640625e-07\n",
      "x is 2.0, y is 4.0，l is 9.094947017729282e-13\n",
      "w is 1.9999995231628418, w grad is -3.814697265625e-06\n",
      "x is 3.0, y is 6.0，l is 2.0463630789890885e-12\n",
      "w is 1.9999995231628418, w grad is -8.58306884765625e-06\n",
      "Epoch: 50\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 51\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 52\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 53\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 54\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 55\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 56\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 57\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 58\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 59\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 60\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 61\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 62\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 63\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 64\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 65\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 66\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 67\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 68\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 69\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 70\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 71\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 72\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 73\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 74\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 75\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 76\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 77\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 78\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 79\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 80\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 81\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 82\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 83\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 84\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 85\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 86\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 87\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 88\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 89\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 90\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 91\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 92\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 93\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 94\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 95\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 96\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 97\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 98\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Epoch: 99\n",
      "x is 1.0, y is 2.0，l is 1.2789769243681803e-13\n",
      "w is 1.9999996423721313, w grad is -7.152557373046875e-07\n",
      "x is 2.0, y is 4.0，l is 5.115907697472721e-13\n",
      "w is 1.9999996423721313, w grad is -2.86102294921875e-06\n",
      "x is 3.0, y is 6.0，l is 9.094947017729282e-13\n",
      "w is 1.9999996423721313, w grad is -5.7220458984375e-06\n",
      "Predict(after training) 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.Tensor([1.0])  # 初始权值\n",
    "w.requires_grad = True  # 设置为需要计算梯度\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    y_pred = x * w\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    l = (y_pred - y) ** 2\n",
    "    return l\n",
    "\n",
    "\n",
    "def optim(w):\n",
    "    w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "\n",
    "print('Predict (befortraining)', 4, forward(4).item())\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('Epoch:', epoch)\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        if w.grad is not None:\n",
    "            w.grad.data.zero_()  # 等价于 optimizer.zero_grad()\n",
    "        y_pred = forward(x)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        print(f'x is {x}, y is {y}，l is {l.item()}')\n",
    "        print(f'w is {w.item()}, w grad is {w.grad.item()}')\n",
    "        optim(w)  # 等价于 optimizer.step() 的作用\n",
    "\n",
    "print('Predict(after training)', 4, forward(4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(100, 300),\n",
    "                  nn.GELU(),\n",
    "                  nn.Linear(300, 100),\n",
    "                  nn.Dropout(0.25)\n",
    "                  )\n",
    "\n",
    "    def forward(self, inputs, target=None):\n",
    "        x = inputs\n",
    "        outputs = self.layers(x)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "test_net = TestNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7567524e79b96d316162151a38259d28be4bc298929f85b9103bf7fbca02c0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
