{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5183378112 5162402656\n",
      "4358384768 4358787008\n",
      "True True\n",
      "tensor(2.)\n",
      "None tensor(5.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22502/3133686898.py:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(b.grad, a.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a.clone()\n",
    "# a和b不是同一个对象\n",
    "print(id(a), id(b))\n",
    "# 也不指向同一块内存地址\n",
    "print(a.data_ptr(), b.data_ptr())\n",
    "# 但b的requires_grad属性和a的一样，同样是True\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "\n",
    "c = a * 2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "\n",
    "b = b + 2\n",
    "d = b * 3\n",
    "d.backward()\n",
    "# b是普通节点而非叶子结点梯度值不会被保存, 但是b在计算图中，其梯度会传递给a\n",
    "print(b.grad, a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4401773920 4401773920\n",
      "4952537088 4952537088\n",
      "True True\n",
      "tensor(2.)\n",
      "tensor(5.) tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a\n",
    "print(id(a), id(b))\n",
    "print(a.data_ptr(), b.data_ptr())\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "\n",
    "c = a * 2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "\n",
    "# b = b + 2\n",
    "d = b * 3\n",
    "d.backward()\n",
    "print(b.grad, a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8808, grad_fn=<SelfDefinedSigmoidBackward>) Parameter containing:\n",
      "tensor(2., requires_grad=True) tensor(0.1050)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SelfDefinedSigmoid(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        result = torch.divide(torch.tensor(1), (1 + torch.exp(-inp)))\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # ctx.saved_tensors is tuple (tensors, grad_fn)\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result * (1 - result)\n",
    "\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = SelfDefinedSigmoid.apply(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "sg = Sigmoid()\n",
    "inputs = torch.nn.Parameter(torch.tensor(2.))\n",
    "outputs = sg(inputs)\n",
    "outputs.backward(torch.ones_like(inputs), retain_graph=True)\n",
    "\n",
    "print(outputs, inputs, inputs.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# 冒泡排序\n",
    "rand_list = [0, 1, 4, 3, 2, 5, 6, 7, 9, 8]\n",
    "n = len(rand_list)\n",
    "for i in range(1, n):\n",
    "    for j in range(0, n - i):\n",
    "        if rand_list[j] > rand_list[j + 1]:\n",
    "            rand_list[j], rand_list[j + 1] = rand_list[j + 1], rand_list[j]\n",
    "print(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "fig = plot.figure(1, figsize=(2, 2), dpi=300)\n",
    "image = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]])\n",
    "plot.imshow(image, cmap='gray')\n",
    "plot.margins(0., 0.)  # 让轴中的内容紧贴轴\n",
    "plot.axis('off')\n",
    "plot.subplots_adjust(0., 0., 1., 1., 0., 0.)  # 让全部轴所处的区域与图完全重合\n",
    "fig.savefig('image.tiff', bbox_inches='tight', pad_inches=0., transparent=True)\n",
    "plot.show()\n",
    "plot.close(fig=fig)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7567524e79b96d316162151a38259d28be4bc298929f85b9103bf7fbca02c0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
