{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a data = (tensor(2.), True), grad = None\n",
      "b data = (tensor(3.), False), grad = None\n",
      "c data = (tensor(6.), True), grad = None\n",
      "d data = (tensor(4.), True), grad = None\n",
      "e data = (tensor(24.), False), grad = None\n",
      "f data = (tensor(24.), True), grad = None\n",
      "g data = (tensor(5.), False), grad = None\n",
      "h data = (tensor(120.), True), grad = None\n",
      "i data = (tensor(144.), True), grad = None\n",
      "a data = (tensor(2.), True), grad = 60.0\n",
      "b data = (tensor(3.), False), grad = None\n",
      "c data = (tensor(6.), True), grad = None\n",
      "d data = (tensor(4.), True), grad = 30.0\n",
      "e data = (tensor(24.), False), grad = None\n",
      "f data = (tensor(24.), True), grad = None\n",
      "g data = (tensor(5.), False), grad = None\n",
      "h data = (tensor(120.), True), grad = 1.0\n",
      "i data = (tensor(144.), True), grad = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:22: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"h data = {h.data,h.requires_grad}, grad = {h.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:25: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:32: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:35: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
      "/var/folders/jb/g3ylc11574sd3g6w3t57g5rm0000gn/T/ipykernel_22880/1565945853.py:38: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662188943573/work/build/aten/src/ATen/core/TensorBody.h:483.)\n",
      "  print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.nn.Parameter(torch.tensor(2.))\n",
    "b = torch.tensor(3.)\n",
    "c = a * b\n",
    "d = torch.nn.Parameter(torch.tensor(4.))\n",
    "e = c * d\n",
    "# e.requires_grad_(False)\n",
    "e.detach_()\n",
    "f = c * d\n",
    "# f.detach_()\n",
    "g = torch.nn.Parameter(torch.tensor(5.))\n",
    "g.requires_grad_(False)\n",
    "h = f * g\n",
    "i = e + h\n",
    "\n",
    "print(f\"a data = {a.data,a.requires_grad}, grad = {a.grad}\")\n",
    "print(f\"b data = {b.data,b.requires_grad}, grad = {b.grad}\")\n",
    "print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
    "print(f\"d data = {d.data,d.requires_grad}, grad = {d.grad}\")\n",
    "print(f\"e data = {e.data,e.requires_grad}, grad = {e.grad}\")\n",
    "print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
    "print(f\"g data = {g.data,g.requires_grad}, grad = {g.grad}\")\n",
    "print(f\"h data = {h.data,h.requires_grad}, grad = {h.grad}\")\n",
    "print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")\n",
    "\n",
    "h.retain_grad()\n",
    "i.backward()\n",
    "\n",
    "print(f\"a data = {a.data,a.requires_grad}, grad = {a.grad}\")\n",
    "print(f\"b data = {b.data,b.requires_grad}, grad = {b.grad}\")\n",
    "print(f\"c data = {c.data,c.requires_grad}, grad = {c.grad}\")\n",
    "print(f\"d data = {d.data,d.requires_grad}, grad = {d.grad}\")\n",
    "print(f\"e data = {e.data,e.requires_grad}, grad = {e.grad}\")\n",
    "print(f\"f data = {f.data,f.requires_grad}, grad = {f.grad}\")\n",
    "print(f\"g data = {g.data,g.requires_grad}, grad = {g.grad}\")\n",
    "print(f\"h data = {h.data,h.requires_grad}, grad = {h.grad}\")\n",
    "print(f\"i data = {i.data,i.requires_grad}, grad = {i.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7567524e79b96d316162151a38259d28be4bc298929f85b9103bf7fbca02c0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
