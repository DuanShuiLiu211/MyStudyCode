{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. ***用于二分类任务或重建任务，loss function bce***\n",
    "***F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)***\n",
    "- weight (Tensor, optional): a manual rescaling weight given to the loss of each batch element.\n",
    "- This is used for measuring the error of a reconstruction in for example an auto-encoder.\n",
    "\n",
    "***The unreduced (i.e. with `reduction` set to ``'none'``) loss can be described as***\n",
    "\n",
    "$$\n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n",
    "$$\n",
    "\n",
    "- `N` is the batch size. \n",
    "- targets `y` should be numbers between 0 and 1.\n",
    "\n",
    "***If `reduction` is not ``'none'`` (default ``'mean'``), then***\n",
    "\n",
    "$$\n",
    "\\ell(x, y) = \\begin{cases}\n",
    "\\operatorname{mean}(L), \\quad & \\text{if reduction} = \\text{`mean';}\\\\\n",
    "\\operatorname{sum}(L), \\quad & \\text{if reduction} = \\text{`sum'.}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bce = nn.BCELoss()\n",
    "\n",
    "def binary_cross_entorpy(inputs, targets):\n",
    "    inputs = inputs.numpy()\n",
    "    inputs = inputs.reshape((inputs.shape[0]*inputs.shape[1], inputs.shape[-2]*inputs.shape[-1]))\n",
    "    targets = targets.numpy()\n",
    "    targets = targets.reshape((targets.shape[0]*targets.shape[1], targets.shape[-2]*targets.shape[-1]))\n",
    "    outputs = 0.\n",
    "    weight = 1.\n",
    "    for i in range(targets.shape[0]):\n",
    "        temp = 0\n",
    "        for j in range(targets.shape[1]):\n",
    "            temp += -1. * weight * (targets[i, j]*np.log(inputs[i, j]) + (1-targets[i, j])*np.log(1-inputs[i, j]))\n",
    "        outputs += (temp / targets.shape[1])\n",
    "        \n",
    "    return outputs / targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4346, 0.9757],\n",
      "          [0.5365, 0.6144]],\n",
      "\n",
      "         [[0.2446, 0.2691],\n",
      "          [0.7291, 0.4737]]]])\n",
      "tensor([[[[0.5867, 0.4917],\n",
      "          [0.5620, 0.5491]],\n",
      "\n",
      "         [[0.4133, 0.5083],\n",
      "          [0.4380, 0.4509]]]])\n",
      "bce 0.892566\n",
      "binary_cross_entorpy 0.892566\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand((1, 2, 2, 2))\n",
    "outputs = torch.tensor([[[[0, 1.], [1., 0]], [[0, 1.], [1., 0]]]])\n",
    "# outputs = torch.nn.Softmax(dim=1)(torch.rand(1, 2, 2, 2))\n",
    "\n",
    "print(inputs,\n",
    "      outputs,\n",
    "      f'bce {bce(inputs, outputs):6f}',\n",
    "      f'binary_cross_entorpy {binary_cross_entorpy(inputs, outputs):6f}',\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. ***用于多分类任务，loss function ce***\n",
    "***F.cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction, label_smoothing=self.label_smoothing)***\n",
    "- this case is equivalent to the combination of `~torch.nn.LogSoftmax` and `~torch.nn.NLLLoss`.\n",
    "  > $$\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)$$\n",
    "  > $$Nll(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$$\n",
    "- It is useful when training a classification problem with `C` classes.\n",
    "- If provided, the optional argument `weight` should be a 1D `Tensor` assigning weight to each of the classes, This is\n",
    "particularly useful when you have an unbalanced training set.\n",
    "\n",
    "***The `input` is expected to contain raw, unnormalized scores for each class.***\n",
    "- `input` has to be a Tensor of size `(C)` for unbatched input, `(N, C)` or `(N, C, d_1, d_2, ..., d_K)` with $K\\geq 1$ for the `K`-dimensional case.\n",
    "  > $$\\begin{aligned}\n",
    "      C ={} & \\text{number of classes} \\\\\n",
    "      N ={} & \\text{batch size} \\\\\n",
    "      \\end{aligned}$$\n",
    "\n",
    "***The `target` that this criterion expects should contain either***\n",
    "- Class indices in the range `[0, C)` where `C` is the number of classes, not one-hot, dtype is long.\n",
    "- if `ignore_index` is specified, loss also accepts this class index (this index may not necessarily be in the class range).\n",
    "- If containing class probabilities, same shape as the input and each value should be between `[0, 1]`, dtype is float.\n",
    "- The unreduced (i.e. with `reduction` set to ``'none'``) loss for this case can be described as\n",
    "\n",
    "$$\n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
    "\\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
    "$$\n",
    "\n",
    "`x` is the input, `y` is the target, `w` is the weight,\n",
    "`C` is the number of classes, and `N` spans the minibatch dimension as well as `d_1, ..., d_k` for the `K`-dimensional case.\n",
    "\n",
    "- The performance of this criterion is generally better when `target` contains class\n",
    "  indices, as this allows for optimized computation. Consider providing `target` as\n",
    "  class probabilities only when a single class label per minibatch item is too restrictive.\n",
    "\n",
    "***The `output` If reduction is 'none', same shape as the target. Otherwise, scalar.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()\n",
    "ls = nn.LogSoftmax(dim=1)\n",
    "nll = nn.NLLLoss()\n",
    "\n",
    "\n",
    "def cross_entorpy(inputs, targets):\n",
    "    inputs = inputs.numpy()\n",
    "    targets = targets.numpy()\n",
    "    outputs = 0.\n",
    "    weight = 1.\n",
    "    if targets.dtype == np.int64:\n",
    "        assert len(inputs.shape) == 4 and len(targets.shape) == 3\n",
    "        for k in range(targets.shape[0]):\n",
    "            temp = 0.\n",
    "            for i in range(targets.shape[-2]):\n",
    "                for j in range(targets.shape[-1]):\n",
    "                    temp += -1. * weight * (np.log(np.exp(inputs[k, :, i, j][..., int(targets[k, i, j].item())]) /\n",
    "                            np.sum(np.exp(inputs[k, :, i, j]))))\n",
    "            outputs += temp\n",
    "    elif targets.dtype == np.float32:\n",
    "        assert inputs.shape == targets.shape\n",
    "        for k in range(targets.shape[0]):\n",
    "            temp = 0.\n",
    "            for i in range(targets.shape[-2]):\n",
    "                for j in range(targets.shape[-1]):\n",
    "                    temp += -1. * weight * np.sum(np.log(np.exp(inputs[k, :, i, j]) / np.sum(np.exp(inputs[k, :, i, j]))) * targets[k, :, i, j])\n",
    "            outputs += temp\n",
    "    else:\n",
    "        print(f'标签的数据类型应该是 int64 或者 float32 而不是 {targets.dtype}')\n",
    "        sys.exit()\n",
    "\n",
    "    return (outputs / (targets.shape[0] * targets.shape[-2] * targets.shape[-1])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce 0.725609\n",
      "logsoftmax+nll 0.725609\n",
      "cross_entorpy 0.725609\n"
     ]
    }
   ],
   "source": [
    "# 交叉熵的计算模式 - 标签中的元素是类的索引值, [0, C-1] -> int64\n",
    "inputs = torch.rand(2, 2, 5, 5)\n",
    "targets = torch.rand(2, 5, 5).random_(2).long()\n",
    "\n",
    "# # 交叉熵的计算模式 - 标签中的元素是类的概率值, [0, 1] -> float32\n",
    "# inputs = torch.rand(2, 2, 5, 5)\n",
    "# targets = torch.nn.Softmax(dim=1)(torch.rand(2, 2, 5, 5))\n",
    "\n",
    "outputs = ce(inputs, targets)\n",
    "print(f'ce {outputs:6f}')\n",
    "\n",
    "if targets.dtype == torch.int64:\n",
    "    outputs = nll(ls(inputs), targets)\n",
    "    print(f'logsoftmax+nll {outputs:6f}')\n",
    "\n",
    "outputs = cross_entorpy(inputs, targets)\n",
    "print(f'cross_entorpy {outputs:6f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7567524e79b96d316162151a38259d28be4bc298929f85b9103bf7fbca02c0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
