{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Jax 版本0.x阶段，神经网络模型标准构建流程\n",
    "Jax包含自微分机制（Autograd）和加速线性代数库（XLA）\n",
    " - 适用于是高性能机器学习研究。JAX可以自动区分本机Python和NumPy函数。它可以通过循环、分支、递归和闭包求导，还可以求导数的导数的导数。\n",
    " - 可使用神经网络库（Flax）灵活的构建深度学习模型，可以任意顺序组合梯度的反向模差分(又称反向传播)和正向模差分，梯度通过 Optax 实现优化与处理\n",
    " \n",
    "Jax训练深度学习模型的逻辑\n",
    " - trained_weight <- model, init_weight, data\n",
    " - predict_label <- model(weight_after_train, predict_to_data)\n",
    "\n",
    "Jax的环境部署\n",
    "- !pip install jax\n",
    "- !pip install jaxlib\n",
    "- !pip install flax\n",
    "- !pip install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Sequence, Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"ResNet block.\"\"\"\n",
    "    ModuleDef = Any\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, ):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (3, 3), self.strides)(x)\n",
    "        y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3, 3))(y)\n",
    "        y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "\n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters, (1, 1),\n",
    "                                 self.strides, name='conv_proj')(residual)\n",
    "            residual = self.norm(name='norm_proj')(residual)\n",
    "\n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class BottleneckResNetBlock(nn.Module):\n",
    "    \"\"\"Bottleneck ResNet block.\"\"\"\n",
    "    ModuleDef = Any\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (1, 1))(x)\n",
    "        y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3, 3), self.strides)(y)\n",
    "        y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters * 4, (1, 1))(y)\n",
    "        y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "\n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters * 4, (1, 1),\n",
    "                                 self.strides, name='conv_proj')(residual)\n",
    "            residual = self.norm(name='norm_proj')(residual)\n",
    "\n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNetV1.\"\"\"\n",
    "    ModuleDef = Any\n",
    "    stage_sizes: Sequence[int]\n",
    "    block_cls: ModuleDef\n",
    "    num_classes: int\n",
    "    num_filters: int = 64\n",
    "    dtype: Any = jnp.float32\n",
    "    act: Callable = nn.relu\n",
    "    conv: ModuleDef = nn.Conv\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        conv = partial(self.conv, use_bias=False, dtype=self.dtype)\n",
    "        norm = partial(nn.BatchNorm,\n",
    "                       use_running_average=not train,\n",
    "                       momentum=0.9,\n",
    "                       epsilon=1e-5,\n",
    "                       dtype=self.dtype)\n",
    "\n",
    "        x = conv(self.num_filters, (7, 7), (2, 2),\n",
    "                 padding=[(3, 3), (3, 3)],\n",
    "                 name='conv_init')(x)\n",
    "        x = norm(name='bn_init')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, (3, 3), strides=(2, 2), padding='SAME')\n",
    "        for i, block_size in enumerate(self.stage_sizes):\n",
    "            for j in range(block_size):\n",
    "                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n",
    "                x = self.block_cls(self.num_filters * 2 ** i,\n",
    "                                   strides=strides,\n",
    "                                   conv=conv,\n",
    "                                   norm=norm,\n",
    "                                   act=self.act)(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n",
    "        x = jnp.asarray(x, self.dtype)\n",
    "        return x\n",
    "\n",
    "\n",
    "ResNet18_c10 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n",
    "                       block_cls=ResNetBlock, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义数据流\n",
    "def data_flow(*, dataset, batch_size=1, prng=None):\n",
    "    total_data = len(dataset)\n",
    "    if prng is not None:\n",
    "        index_order = np.array(range(total_data))\n",
    "        index_shuffle = jax.random.permutation(prng, index_order, independent=True)\n",
    "    else:\n",
    "        index_order = np.array(range(total_data))\n",
    "        index_shuffle = index_order\n",
    "\n",
    "    total_batch = total_data // batch_size\n",
    "    for idx in range(total_batch):\n",
    "        batch_index = index_shuffle[idx * batch_size: (idx + 1) * batch_size]\n",
    "        mini_batch = [dataset[k] for k in batch_index]\n",
    "        images = np.expand_dims(np.stack([x['image'] for x in mini_batch]), -1).astype('float') / 255\n",
    "        labels = np.stack([x['label'] for x in mini_batch])\n",
    "        yield {\n",
    "            'image': images,\n",
    "            'label': labels\n",
    "        }\n",
    "\n",
    "\n",
    "dataset_mnist = np.load(\"dataset/mnist.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义评估指标\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化模型进入训练状态\n",
    "def create_train_state(*, prng, learning_rate, momentum):\n",
    "    net = ResNet18_c10()\n",
    "    params = net.init(prng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(apply_fn=net.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义训练方法\n",
    "# 定义训练的每步操作\n",
    "@jax.jit\n",
    "def train_step(state, batch_data):\n",
    "    \"\"\"\n",
    "    state: 不仅包含参数信息还包含优化器的信息等\n",
    "    batch_data: 批数据 (N, H, W, C)\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits, _ = ResNet18_c10().apply({'params': params}, batch_data['image'], mutable=['batch_stats'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch_data['label'])\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch_data['label'])\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "# 定义训练的执行逻辑\n",
    "def train_model(state, epoch, batch_size, prng):\n",
    "    batch_metrics = []\n",
    "    train_dataset = dataset_mnist['train']\n",
    "    total_batch = len(train_dataset) // batch_size\n",
    "\n",
    "    with tqdm(data_flow(dataset=train_dataset, batch_size=batch_size, prng=prng), total=total_batch) as run_bar_set:\n",
    "        for batch in run_bar_set:\n",
    "            state, metrics = train_step(state, batch)\n",
    "            batch_metrics.append(metrics)\n",
    "            batch_metrics_jnp = jax.device_get(batch_metrics)\n",
    "            epoch_metrics = {k: np.mean([metrics[k] for metrics in batch_metrics_jnp]) for k in metrics.keys()}\n",
    "            run_bar_set.set_description(f\"train epoch: {epoch+1}, \"\n",
    "                                        f\"loss: {epoch_metrics['loss']:.4f}, \"\n",
    "                                        f\"accuracy: {(epoch_metrics['accuracy'] * 100):.2f}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义测试方法\n",
    "# 定义测试的每步操作\n",
    "@jax.jit\n",
    "def test_step(params, batch_data):\n",
    "    \"\"\"\n",
    "    params: 经过训练的参数\n",
    "    batch_data: 批数据 (N, H, W, C)\n",
    "    \"\"\"\n",
    "    logits, _ = ResNet18_c10().apply({'params': params}, batch_data['image'], mutable=['batch_stats'])\n",
    "    return compute_metrics(logits=logits, labels=batch_data['label'])\n",
    "\n",
    "\n",
    "# 定义测试执行逻辑\n",
    "def test_model(params, epoch, batch_size):\n",
    "    batch_metrics = []\n",
    "    test_dataset = dataset_mnist['test']\n",
    "    total_batch = len(test_dataset) // batch_size\n",
    "\n",
    "    with tqdm(data_flow(dataset=test_dataset, batch_size=batch_size), total=total_batch) as run_bar_set:\n",
    "        for batch in run_bar_set:\n",
    "            metrics = test_step(params, batch)\n",
    "            batch_metrics.append(metrics)\n",
    "            batch_metrics_jnp = jax.device_get(batch_metrics)\n",
    "            epoch_metrics = {k: np.mean([metrics[k] for metrics in batch_metrics_jnp]) for k in metrics.keys()}\n",
    "            run_bar_set.set_description(f\"train epoch: {epoch+1}, \"\n",
    "                                        f\"loss: {epoch_metrics['loss']:.4f}, \"\n",
    "                                        f\"accuracy: {(epoch_metrics['accuracy'] * 100):.2f}\")\n",
    "\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 进行训练与测试\n",
    "seed = 51\n",
    "prng = jax.random.PRNGKey(seed)  # 通过种子获取随机数生成器密钥\n",
    "prng, init_prng = jax.random.split(prng, 2)  # 拆分原随机数生成器密钥得到2个新的密钥，使用相同密钥随机函数将输出相同结果，用其实现可复现的权重初始化\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "state = create_train_state(prng=init_prng, learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 定义用于打乱数据顺序的伪随机数生成器\n",
    "    prng, data_prng = jax.random.split(prng)\n",
    "    for train_batch_data in data_flow(dataset=dataset_mnist['train'], batch_size=batch_size, prng=data_prng):\n",
    "        print(train_batch_data['image'].shape, train_batch_data['image'].dtype)\n",
    "        print(train_batch_data['label'].shape, train_batch_data['label'].dtype)\n",
    "        break\n",
    "    for test_batch_data in data_flow(dataset=dataset_mnist['test'], batch_size=batch_size):\n",
    "        print(test_batch_data['image'].shape, test_batch_data['image'].dtype)\n",
    "        print(test_batch_data['label'].shape, test_batch_data['label'].dtype)\n",
    "        break\n",
    "    # 训练模型\n",
    "    state = train_model(state, epoch, batch_size, data_prng)\n",
    "    # 测试模型\n",
    "    params = state.params\n",
    "    metrics = test_model(params, epoch, batch_size)\n",
    "\n",
    "print(\"运行完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7567524e79b96d316162151a38259d28be4bc298929f85b9103bf7fbca02c0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
